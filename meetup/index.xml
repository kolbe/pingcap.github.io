<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PingCAP.com</title>
    <link>https://pingcap.com/meetup/</link>
    <description>Recent content on PingCAP.com</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Apr 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://pingcap.com/meetup/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【Infra Meetup No.97】What&#39;s New in TiDB 3.0 &amp; An Introduction to Failpoint Design</title>
      <link>https://pingcap.com/meetup/meetup-97-20190420/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-97-20190420/</guid>
      <description>Topic 1：What&amp;rsquo;s New in TiDB 3.0  讲师介绍：申砾，PingCAP 技术 VP。
  视频 | Infra Meetup No.97：What&amp;rsquo;s New in TiDB 3.0 PPT 链接  1 月 19 日，TiDB 发布 3.0 Beta 版，相比 2.1 版本，该版本对系统稳定性、优化器、统计信息以及执行引擎做了很多改进。申砾老师为大家分享了 TiDB 3.0 的新特性及未来的规划。
Topic 2：An Introduction to Failpoint Design  讲师介绍：龙恒，TiDB SQL Infra Team 开发工程师，主要工作是 TiDB-Lightning / TiKV-Importer 的维护和新功能开发，致力于性能和稳定性提升。
  视频 | Infra Meetup No.97：An Introduction to Failpoint Design PPT 链接  本次分享龙恒老师首先介绍了 Failpoint 的使用场景，以及 github.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.98】Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask</title>
      <link>https://pingcap.com/meetup/meetup-98-20190420/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-98-20190420/</guid>
      <description>讲师介绍：徐怀宇，TiDB 研发工程师，目前主要负责查询执行引擎相关工作。
  视频 | Infra Meetup No.98：Compiled and Vectorized Queries PPT 链接  本次分享徐怀宇老师为大家介绍了论文《Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to Ask》，主要包括：
 介绍经典 Volcano 模型的执行流程，并分析其运行时性能。
 介绍行存、列存的基本概念，并进而引出向量化执行，分析其如何克服经典 Volcano 模型的缺点。
 介绍代码生成的基本概念，结合案例分析其如何克服经典 Volcano 模型的缺点。
  最后，结合论文内容，重点从 micro-architecture, data-parallel execution 两个方面，分析对比向量化执行和代码生成的特性，进而引出论文结论：向量化执行在 memory-bound 类的查询中更有优势，代码生成在 calculation-heavy 类的查询中更有优势。但是总体来看，在 OLAP 场景中，向量化执行和代码生成的执行性能相近。
延伸阅读 ：
 论文链接   PingCAP Infra Meetup
作为一个基础架构领域的前沿技术公司，PingCAP 希望能为国内真正关注技术本身的 Hackers 打造一个自由分享的平台。自 2016 年 3 月 5 日开始，我们定期在周末举办 Infra Meetup，与大家深度探讨基础架构领域的前瞻性技术思考与经验，目前已在北京、上海、广州、成都、杭州等地举办。在这里，我们希望提供一个高水准的前沿技术讨论空间，让大家真正感受到自由的开源精神魅力。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.99】数据中台 &amp; WiredTiger 引擎实现原理 &amp; JIT in Databases</title>
      <link>https://pingcap.com/meetup/meetup-99-20190420/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-99-20190420/</guid>
      <description>Topic 1: 宝尊对数据中台搭建的思考与探索  讲师介绍：张建，宝尊电商技术总监。
 数据中台最近特别火，很多企业都在关注如何构建自己的数据中台，利用数据中台打造数据驱动的经营能力。同时，数据中台的概念也漫天飞，但是很难有一个大家都认同的标准。一个有趣的现象是数据中台在国内数据圈子里在升温，但是国外却鲜有提及，以至于我们在向 Gartner 咨询相关主题的时候，国外的咨询师都一头雾水，不知道数据中台是什么。甚至想找一个对“数据中台”比较恰当的英文翻译都很难。
面对这个既新又杂的概念，宝尊也在进行自己的“数据中台”探索。本次研讨会，宝尊的算法和大数据部负责人张建就自己对数据中台的调研和思考，与大家一起开脑洞，共同研究和探讨了数据中台是什么，它和数据仓库有什么区别，数据中台的核心价值是什么等主题。
 应讲师要求，该分享视频&amp;amp;PPT 资料仅限内部学习交流，不对外公开～
 Topic 2：WiredTiger 引擎实现原理  讲师介绍：许鹏，携程机票技术总监，负责机票大数据基础平台的架构和运维，《Apache Spark 源码剖析》一书作者，三年 Presto 及 Elasticsearch，MongoDB 集群的一线运维经验。长期专注于分布式计算引擎技术和分布式存储的设计与实现。
  视频 | Infra Meetup No.99：WiredTiger 引擎实现原理 PPT 链接  WiredTiger 作为 MongoDB 的默认存储引擎，许鹏从整体架构，内存管理，磁盘寻址，数据持久化，文件压缩最佳实践等维度介绍 WiredTiger，并描述如何最小化线程间的竞争，如何充分现代计算机平台中的多核和大内存的优势，在 WiredTiger 并发控制机制中的体现，最后也介绍从源码级别怎么分析和调试 WiredTiger。
Topic 3：JIT in Databases  讲师介绍：吴逸飞，TiSpark 研发工程师。
  视频 | Infra Meetup No.99：WiredTiger 引擎实现原理 PPT 链接  本次分享内容主要包括：
 介绍了 JIT (即时编译技术) 在数据库中的意义：
 避免传统解释系统的无关开销。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.96】Introduction to Titan</title>
      <link>https://pingcap.com/meetup/meetup-96-20190413/</link>
      <pubDate>Sat, 13 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-96-20190413/</guid>
      <description>在上周六举办的 Infra Meetup No.96 上，我司 TiKV 研发工程师张博康为大家介绍了我们自研的高性能单机 key-value 存储引擎 Titan，以下是视频 &amp;amp; 文字回顾，enjoy～
 讲师介绍：张博康，TiKV 研发工程师，目前负责 TiKV raftstore 以及存储引擎相关工作。
  视频 | Infra Meetup No.96：Introduction to Titan PPT 链接  本次分享的主要内容包括：
 分析 LSM-Tree 的写放大问题，以阐述 Titan 核心的思路——key-value 分离。
 从设计目标出发，介绍并对比了 Wisckey 和 Titan 的结构差异。
 介绍了 Titan 的具体设计与实现，包括如何通过 RocksDB 的 TableBuilder，TableProperties，EventListener，WriteCallback 等现有机制实现 key-value 的分离以及 Titan 的 GC 流程。
 展示了 Titan 与 RocksDB 在大 value 情况下的性能对比。
  延展阅读 ：</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.94】TiFlash、Spark SQL</title>
      <link>https://pingcap.com/meetup/meetup-94-20190403/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-94-20190403/</guid>
      <description>Topic 1：TiDB 与 TiFlash 扩展 - 向真 HTAP 平台前进  讲师介绍：孙若曦，PingCAP 核心开发工程师，负责 OLAP 相关产品设计和开发。曾在星环科技、NVIDIA 就职担任 Tech Lead。主要研究分布式系统、数据库等领域。
  视频 | Infra Meetup No.94：TiDB 与 TiFlash 扩展 - 向真 HTAP 平台前进 PPT 链接  本次分享的内容主要包括以下三个方面：
 HTAP 的核心价值：能够解决当前各类数据平台上广泛存在的工具链过于复杂，运维成本高，数据实效性和一致性等问题。
 HTAP 面临的技术挑战：OLTP 场景通常使用行存，而 OLAP 场景通常使用列存；另外，OLAP 任务因为对系统资源占用较多，也会严重影响 OLTP 业务。
 TiFlash 是如何解决这些问题的：
 使用列存及向量化计算来满足 OLAP 业务； 数据使用 Raft Learner 机制同步到列存； 拥有与 TiDB 相同的 Scalability； OLTP 与 OLAP 的物理资源完全隔离，避免互相干扰； TiDB/TiSpark 能够同时访问行存和列存副本，通过 CBO 选取最优化的访问方式； 为 TiFlash 引入 MPP 能力。   Topic 2：eBay 在 Spark SQL 的性能优化  讲师介绍：王刚，eBay 大数据工程师。2017 年硕士毕业于南京大学，后一直在 eBay 从事大数据研发工作。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.95】Introduction of TiDB SQL Layer</title>
      <link>https://pingcap.com/meetup/meetup-95-20190403/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-95-20190403/</guid>
      <description>在上周举办的成都 · Infra Meetup No.95 上，我司 TiDB SQL Engine 开发工程师姚珂男为大家介绍 TiDB SQL 层的技术原理，以下是视频&amp;amp;文字回顾，enjoy～
 讲师介绍：姚珂男，TiDB SQL Engine 开发工程师，主要工作为优化器及相关模块的维护和新功能开发，致力于提升查询计划的正确性和稳定性。
  视频 | Infra Meetup No.95：Introduction of TiDB SQL Layer PPT 链接  本次分享主要介绍 TiDB SQL 层的三个组件：优化器，统计信息和执行引擎。
 优化器部分主要举例介绍了逻辑优化规则和物理优化框架； 统计信息部分主要介绍直方图，CMSketch 以及使用方法； 执行引擎部分以两种 join 方式为例介绍了我们在执行引擎实现中用到的一些优化方法。   PingCAP Infra Meetup
作为一个基础架构领域的前沿技术公司，PingCAP 希望能为国内真正关注技术本身的 Hackers 打造一个自由分享的平台。自 2016 年 3 月 5 日开始，我们定期在周末举办 Infra Meetup，与大家深度探讨基础架构领域的前瞻性技术思考与经验，目前已在北京、上海、广州、成都、杭州等地举办。在这里，我们希望提供一个高水准的前沿技术讨论空间，让大家真正感受到自由的开源精神魅力。
 </description>
    </item>
    
    <item>
      <title>【Infra Meetup No.92】Introduction to TiDB Statistics</title>
      <link>https://pingcap.com/meetup/meetup-92-20190327/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-92-20190327/</guid>
      <description>在上周六北京举办的 Infra Meetup No.92 上，我司 TiDB 开发工程师谢海滨老师为大家介绍了 TiDB 中统计信息的原理及细节。以下是视频 &amp;amp; 文字回顾，enjoy！
 讲师介绍：谢海滨，TiDB 研发工程师，目前主要负责统计信息以及优化器相关工作。
  视频 | Infra Meetup No.92：Introduction to TiDB Statistics
 PPT 链接
  本次分享首先介绍了统计信息的作用以及 TiDB 统计信息的基本组成部分，接下来围绕着统计信息的估算、收集以及更新 3 个部分具体展开：
 在估算方面，介绍了直方图和 Count-Min Sketch 的适用场景以及估算方法，以及 TiDB 是如何利用索引的统计信息来减少多列估算时的独立性假设。
 在收集方面，介绍了 analyze 语句的具体流程以及相关参数，以及 auto analyze 的触发条件。
 在更新方面，介绍了 TiDB 是如何更新 row count 和 modify count，以及是如何利用查询结果更新直方图和 Count-Min Sketch 的。
   PingCAP Infra Meetup
作为一个基础架构领域的前沿技术公司，PingCAP 希望能为国内真正关注技术本身的 Hackers 打造一个自由分享的平台。自 2016 年 3 月 5 日开始，我们定期在周末举办 Infra Meetup，与大家深度探讨基础架构领域的前瞻性技术思考与经验，目前已在北京、上海、广州、成都、杭州等地举办。在这里，我们希望提供一个高水准的前沿技术讨论空间，让大家真正感受到自由的开源精神魅力。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.93】A Study of LSM-Tree</title>
      <link>https://pingcap.com/meetup/meetup-93-20190327/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-93-20190327/</guid>
      <description>在上周六广州举办的 Infra Meetup No.93 上，来自微信的林金河老师为大家分享了 LSM-Tree 相关知识。以下为视频&amp;amp;文字回顾，enjoy~
 讲师介绍：林金河，目前在微信从事分布式存储相关的工作
  视频 | Infra Meetup No.93：A Study of LSM-Tree
 PPT 链接
  本次分享的主要内容包括：
 LSM-Tree 的基本原理，包括 LSM-Tree 的文件组织结构、Point Query、Range Query 和 Compaction。
 LSM-Tree 存在的问题和相关的优化方法。主要有两方面：
 读放大。目前的优化思路是通过 filter 来减少不必要的 I/O，比如 bloom filter、SuRF。 Compaction 造成的负载抖动和写放大。一方面，可以通过软硬件结合的方式，将 compaction 的任务交给专门的 coprocessor 来做，将 compaction 带来的负面影响尽可能隔离开。另一方面，就是从数据结构和算法上，尽可能减少写放大，比如 PebbleDB 和 WiscKey。  最后简单总结了一下：LSM-Tree 的优化，基本都是在读放大、写放大和空间放大这三者间做 trade-off。理论上没法同时让这三者达到最优（有点像分布式系统的 CAP 定理）。
   PingCAP Infra Meetup
作为一个基础架构领域的前沿技术公司，PingCAP 希望能为国内真正关注技术本身的 Hackers 打造一个自由分享的平台。自 2016 年 3 月 5 日开始，我们定期在周末举办 Infra Meetup，与大家深度探讨基础架构领域的前瞻性技术思考与经验，目前已在北京、上海、广州、成都、杭州等地举办。在这里，我们希望提供一个高水准的前沿技术讨论空间，让大家真正感受到自由的开源精神魅力。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.91】Head First Distributed Transaction in TiDB</title>
      <link>https://pingcap.com/meetup/meetup-91-20190321/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-91-20190321/</guid>
      <description>在上周六举办的 Infra Meetup No.91 上，我司 TiKV 研发工程师吴雪莲老师为杭州小伙伴分享了分布式事务在 TiDB 中实现的原理和细节，以下是视频 &amp;amp; 文字回顾，enjoy！
 讲师介绍：吴雪莲，TiKV 研发工程师，目前主要负责 TiDB/TiKV 事务、TiKV 计算层 Coprocessor 相关研发。
  视频 | Infra Meetup No.91：Head First Distributed Transaction in TiDB
 PPT 链接
  本次分享的主题是分布式事务在 TiDB 中的实现，主要围绕以下三个方面展开：
 分布式事务的定义
 Percolator 中事务的实现
 TiDB 中事务的实现及注意事项
  首先，在分布式事务的定义中，主要介绍了 ACID 和四种常见隔离级别。然后解读了 Percolator 中事务实现，核心内容包括：1. 基于快照隔离级别的优缺点；2. 如何通过两阶段提交实现跨行跨表的分布式事务。
最后，我们详细介绍了 TiDB 中分布式事务的实现，包括 TiDB 如何将关系型数据转化成 key-value 存储，TiDB 中两阶段提交的实现细节及异常处理，以及 TiDB 事务使用过程中的注意事项。
 PingCAP Infra Meetup</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.90】知乎已读服务架构演进</title>
      <link>https://pingcap.com/meetup/meetup-90-20190314/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-90-20190314/</guid>
      <description>在第 90 期 Infra Meetup 上，来自知乎的孙晓光老师为大家分享了知乎已读服务的架构演进的经验。小伙伴们热情爆棚，QA 环节长达 1 小时，快戳视频看看孙老师都分享了哪些有趣的「踩坑经验」吧！
 讲师介绍：孙晓光，知乎搜索工程团队负责人，TiKV Committer。
  视频 | Infra Meetup No.90：知乎已读服务架构演进
 PPT 链接
  孙晓光老师在本期 Meetup 上提到，知乎已读服务的设计严格意义上来说同很多业务向系统的设计有不少差异，而这些差异反映在过程和结果上有些是正向的，也有些是负向的。但是很高兴的看到至少在上线一年多来，整体的收益是远高于所付出的代价的。
最后他希望在近期全量数据迁移到 TiDB 完成后能够进一步解决目前架构的一些尚存的痛点问题，让这个架构跑的更稳跑的更好。
 PingCAP Infra Meetup
作为一个基础架构领域的前沿技术公司，PingCAP 希望能为国内真正关注技术本身的 Hackers 打造一个自由分享的平台。自 2016 年 3 月 5 日开始，我们定期在周末举办 Infra Meetup，与大家深度探讨基础架构领域的前瞻性技术思考与经验，目前已在北京、上海、广州、成都、杭州等地举办。在这里，我们希望提供一个高水准的前沿技术讨论空间，让大家真正感受到自由的开源精神魅力。
 </description>
    </item>
    
    <item>
      <title>【Infra Meetup No.89】TiKV 最新性能优化</title>
      <link>https://pingcap.com/meetup/meetup-89-20190227/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-89-20190227/</guid>
      <description>在上周六举办的 Infra Meetup 上，TiKV 研发工程师屈鹏为大家介绍了 TiKV 最新性能优化。现场讨论非常热烈，分享结束后还有很多小伙伴意犹未尽，留在现场和讲师交流。欢迎大家多来参加 Meetup 感受现场交流的乐趣哦～ 以下是本期 Meetup 的文字 &amp;amp; 视频回顾，enjoy！
 讲师介绍：屈鹏，2017 年加入 PingCAP，TiKV 研发工程师。专注于分布式数据库领域，擅长 Raft 及 TiKV 的性能优化。
  视频 | Infra Meetup No.89：TiKV 最新性能优化
 PPT 链接
  屈鹏老师首先为大家介绍了 TiKV 最新版本的 3 个新的优化：
 batch gRPC/Raft messages 特性可以将消息收集为一个 batch 批量发送，减少了网络相关的系统调用次数，达到了性能上的提升。
 threaded raftstore/apply 特性将之前系统中的两个单线程组件替换为多线程，同时避免了数据倾斜和饥饿，消除了 TiKV 在写入上的瓶颈。
 distributed GC 大幅重构了 GC 相关的代码，GC 的驱动者由客户端变成了 TiKV 自己，简化了客户端的编写难度，同时将 GC 速度加快了 3 倍。
  最后屈鹏老师分享了几个正在开发中的优化，包括事务提交不取 timestamp 等等。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.88】小红书的社区架构 &amp; TiDB 在小红书的实践案例分享 &amp; Vectorized Execution Explained</title>
      <link>https://pingcap.com/meetup/meetup-88-20190116/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-88-20190116/</guid>
      <description> 在上周六举办的 Infra Meetup No.88 上海站上，来自小红书的郭一、张俊骏两位老师，和我司施闻轩老师一起，为大家带来三个精彩的分享，以下是视频&amp;amp;文字回顾，enjoy~
小红书的社区架构  视频 | Infra Meetup No.88：小红书的社区架构  本次分享，郭一老师主要介绍了小红书社区的数据技术。首先介绍了小红书的产品和社区个性化的推荐需要的关键技术。然后对社区的数据技术分别从接入层，业务层，数据服务层和数据仓库层进行了概述。然后讲述了一个利用流计算引擎 Flink 给线上推荐提供用户行为实时的多维度聚合的业务实例。最后对小红书下一年的数据架构发展进行了展望。
TiDB 在小红书的实践案例分享  视频 | Infra Meetup No.88：TiDB 在小红书的实践案例分享  本次分享，张俊骏老师主要介绍了小红书在以下两个场景中对 TiDB 的使用：第一个场景是大促实时看板，在高 QPS 场景下通过最终一致性写入模型完美地满足了需求，且全程保持稳定；第二个是作为分库分表 MySQL 的从库进行 ETL 任务，通过分析分库分表 MySQL 的特性自行开发了同步工具，解决了许多 ETL 任务的痛点。小红书未来还会在 TiDB 的容器化部署、自动化运维、接入更多业务场景等方向上努力。
Vectorized Execution Explained  视频 | Infra Meetup No.88：Vectorized Execution Explained  2019 年我们会尝试针对一些主题进行一系列分享，Q1 计划的是查询执行（Query Execution）相关主题。针对这个主题我们会分享当前业界相对前沿的设计和算法，例如 JIT Compilation，向量化，SIMD，NUMA 相关优化等。
本次 Meetup 施闻轩老师的分享主题是「向量化执行」。向量化是随着列存数据库一起成熟的新查询执行模型，诸如 Hive，Vertica，Vectorwise，Clickhouse 等都使用了该技术。向量化也是 TiDB 正在进行的优化之一。本次分享从为何进行向量化，块执行，SIMD 和晚期物化等多个方面阐述向量化引擎的设计和实现。
 PPT 链接  </description>
    </item>
    
    <item>
      <title>【Infra Meetup No.87】摩拜数据复制中心 Gravity 介绍</title>
      <link>https://pingcap.com/meetup/meetup-87-20190108/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-87-20190108/</guid>
      <description>在上周六举办的 Infra Meetup No.87 上，来自摩拜的胡明老师为大家介绍了摩拜数据复制中心 Gravity，现场讨论氛围非常热烈，来自摩拜的任弘迪老师也解答了大家的一些疑问。以下是现场视频&amp;amp;文字回顾，enjoy~
 视频 | Infra Meetup No.87：摩拜数据复制中心 Gravity 介绍
 PPT 下载链接
  Gravity 是摩拜数据库团队使用 Golang 研发的一款数据同步组件。实现了线上数据库变更的实时推送，MySQL 数据库的单向、双向同步。在数据同步过程中，还支持自定义的数据变换（列映射、重命名等）。Gravity 可以使用单进程的方式部署，也可以使用基于 Kubernetes 的集群模式部署。
在摩拜内部，Gravity 被用在多种场景下，包括大数据总线的建设，分库分表到合库的同步，大规模数据清洗，配合微服务拆分的数据库实时双向同步。
在此次 Meetup 上，大家一起讨论了很多数据同步方面遇到的宝贵经验。包括怎么实现数据库的双向同步，分库分表到合库时 DDL 的处理遇到的坑，在集群模式下可能发生脑裂带来的数据不一致的情况。
最后，胡明老师分享了 Gravity 的 Roadmap，包括对 DDL 的支持，统一的序列化格式，bingo 归档，PostgreSQL 的支持，以及原生的 TiDB 增量同步的支持。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.85】宝尊 &#43; Apache Spark &#43; TiDB SQL Layer</title>
      <link>https://pingcap.com/meetup/meetup-85-20181226/</link>
      <pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-85-20181226/</guid>
      <description>在上周六举办的 Infra Meetup 上海站上，来自宝尊电商和阿里云的资深工程师们，与我司核心开发工程师一起探讨了新一代开源分布式数据库 TiDB 和 Apache Spark 在 SQL 层面的执行原理、优化方案，以及电商数据的技术解决方案等话题。以下是视频&amp;amp;文字回顾，enjoy～
宝尊的 Cloud Native Migration Path 与 TiDB 的应用展望  视频 | Infra Meetup No.85：宝尊的 Cloud Native Migration Path 与 TiDB 的应用展望 PPT 链接  宝尊是知名品牌电子商务商业伙伴和技术研发解决方案公司，拥有端到端数字化零售及供应链管理系统集成解决方案的自主知识产权。宝尊技术与创新中心为了达成“科技驱动商业未来”的宝尊战略愿景，启动全面的云转型战略以满足宝尊科技能力输出的战略目标。
本次 Meetup 上，邵千里老师分享了宝尊在云原生转型中的技术路线选择和技术挑战，同时介绍 TiDB 作为新一代的数据库，在宝尊的科技转型的过程中的使用现状和效果以及宝尊对这一新技术在未来的平台版图中的位置的展望。
Apache Spark 优化机会与探索  视频 | Infra Meetup No.85：Apache Spark 优化机会与探索 PPT 链接  本次分享，李呈祥老师主要介绍了阿里巴巴 EMR 团队在 Spark SQL 方向上的一些优化工作，通过在 Catalyst 和 shuffle 等模块的优化，大幅提高了用户数据处理的性能，最后也介绍了 EMR 在 Spark 优化方向上的一些认识和思考。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.83】What&#39;s New in TiDB 2.1 and What&#39;s Next</title>
      <link>https://pingcap.com/meetup/meetup-83-20181220/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-83-20181220/</guid>
      <description>在 Infra Meetup 第 83 期上，我司 TiDB 核心开发工程师、分布式数据库专家姚维老师为广州的朋友们介绍 TiDB 2.1 的重要特性和未来的规划，以下是视频&amp;amp;文字回顾，enjoy～
 视频 | Infra Meetup No.83：What&amp;rsquo;s New in TiDB 2.1 and What&amp;rsquo;s Next
 PPT 链接
  姚维老师主要介绍了 TiDB 2.1 版本的重要 Feature，包括这些 Feature 所解决的问题、背后的原理、达到的效果，特别是 TiDB 在优化器、计算引擎、存储引擎方面的改进，使得 2.1 版本成为更智能、更迅速、更稳定的数据库。接着展示了部分 Benchmark 结果，分别从 OLAP、OLTP 两个场景表明 TiDB 的性能提升。最后介绍了下一步工作的展望，让大家了解 TiDB 的演进方向。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.84】如何在三分钟内跑完千万测试 case &amp; 硬核 Paper Reading</title>
      <link>https://pingcap.com/meetup/meetup-84-20181220/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-84-20181220/</guid>
      <description>谈谈 TiDB 背后的效率工程：如何在三分钟内跑完千万测试 case  视频 | Infra Meetup No.84：如何在三分钟内跑完千万测试 case
 PPT 链接
  殷成文老师首先介绍了我们在保证 TiDB 正确性以及稳定性上做的一些事情，以及目前遇到的效率的问题， 然后逐个分析目前 CI 慢的原因以及分享我们是如何去解决这些问题的，并介绍了在优化过程中遇到一些坑。 流程上我们结合已有的基础架构选择了 Jenkins with Kubernetes 的方式来解决之前出现的资源调度以及并发上的问题。殷成文老师分享了在使用的过程中遇到的一些坑，以及我们在网络结构上做的调整，提高与 GitHub 交互的速度和稳定性。此外，我们为了减少网络 io 做的两层 cache，减少重复的下载上传操作。最后介绍了我们如何去优化具体 case 以及在优化 TiDB unit test 上用了哪些黑魔法。
Paper Reading  视频 | Infra Meetup No.84：硬核 Paper Reading
 论文《Robust Query Optimization Methods With Respect to Estimation Errors: A Survey》
 PPT 链接
  此次分享谢海滨老师首先介绍了查询优化的相关背景知识，并简单的分析了估算查询代价不准的几个原因。接下来，该分享介绍了三种在查询误差存在情况下的优化方法：
 在执行结束后，我们可以根据查询时得到的真实信息去更新统计信息，以减少之后查询的估算误差。
 在执行过程中，可以物化中间结果，并根据真实的信息重新优化执行计划；或者对于某些查询计划，可以直接调整而不会丢失中间结果。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.81】基于 TiKV 的 Redis 协议兼容层 Titan</title>
      <link>https://pingcap.com/meetup/meetup-81-20181127/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-81-20181127/</guid>
      <description>在 Infra Meeutp No.81 上，来自美图的任勇全老师介绍了美图自研的基于 TiKV 的 Redis 协议兼容层—— Titan 的设计和实现思路。以下是现场视频&amp;amp;文字回顾，enjoy~
 视频 | Infra Meetup No.81：基于 TiKV 的 Redis 协议兼容层 Titan
 PPT 链接
  Titan 是美图基于 TiKV 自主研发的 Redis 协议兼容层，通过将 Redis 丰富的数据类型，映射为 TiKV 中的扁平化的 Key-Value，实现了完整兼容 Redis 协议的分布式存储。
Titan 创新的应用了浮点数作为下标索引实现了LIST 的插入，通过引入对象 ID，结合 GC 机制，实现了大对象的即时删除。另外，Titan 维护了 GC 和过期列表，通过额外的后台线程实现了数据的删除和主动过期。为了解决数据导入的性能瓶颈，Titan 设计并实现了 ZIPLIST，解决了原始设计 KEY 个数放大严重的问题。最后，任老师简单的介绍了 Titan 是如何实现为多个业务提供虚拟化 Redis 集群的多租户机制的。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.82】数据库统计信息的自动挖掘与维护 &amp; What&#39;s New in TiDB</title>
      <link>https://pingcap.com/meetup/meetup-82-20181127/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-82-20181127/</guid>
      <description>上海社区小伙伴们又相聚啦！在 Infra Meetup No.82 上，我司 TiDB 核心开发工程师韩飞、技术 VP 申砾为大家带来了干货分享，以下是现场视频&amp;amp;文字回顾，enjoy~
数据库统计信息的自动挖掘与维护  视频 | Infra Meetup No.82：数据库统计信息的自动挖掘与维护
 PPT 链接
  韩飞老师首先介绍了查询优化器的基本架构与执行流程，并重点介绍了统计信息模块在基于代价的优化（CBO）中的重要作用。在谓词选择率估计（Selectivity Estimation）中，常用的属性独立假设（attribute value independence assumption）在列相关（Column Correlation）的场景下会产生较大误差。
在此次分享上，韩飞老师重点介绍了贝叶斯网络（Bayesian Networks）的解决方案，针对互相依赖的列，使用贝叶斯模型估计依赖关系，并建立多维直方图是一种非常有效的解决方案。另一个影响选择率估计的因素是统计信息的过期问题，根据查询结果的反馈更新直方图信息是一种行之有效的解决方案，但是通常会引入较大误差。通过引入最大熵原则（Max Entropy Principle）可以相对准确的解决直方图更新的问题，这种方法应用在 Informix 商业数据库中。
What&amp;rsquo;s New in TiDB  视频 | Infra Meetup No.82：What&amp;rsquo;s New in TiDB
 PPT 链接
  申砾老师介绍了 TiDB 2.1 版本的重要 Feature，包括这些 Feature 所解决的问题、背后的原理、达到的效果，特别是 TiDB 在优化器、计算引擎、存储引擎方面的改进，使得 2.1 版本成为更智能、更迅速、更稳定的数据库。接着，申砾老师展示了部分 Benchmark 结果，分别从 OLAP、OLTP 两个场景表明 TiDB 的性能提升。最后介绍了下一步工作的展望，让大家了解 TiDB 的演进方向。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.80】从实现角度看 Aurora</title>
      <link>https://pingcap.com/meetup/meetup-80-20181112/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-80-20181112/</guid>
      <description>在Infra Meetup No.80 上，我司 TiDB 核心开发工程师、分布式数据库专家姚维老师为大家分享了 Aurora 相关论文，以下是现场视频&amp;amp;文字回顾，enjoy～
 视频 | Infra Meetup No.80：从实现角度看 Aurora
 PPT 链接
  姚维老师根据论文《Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases》，从实现的角度分析了 Aurora 属于哪一种数据库，Aurora 的读操作、写操作分别是怎么维持数据一致性的，以及 Aurora 如何通过实现 &amp;ldquo;log is the database&amp;rdquo;，使得它拥有高于 MySQL 几十倍的性能优势，也客观的分析了 Aurora 存在的限制与局限。诚然，Aurora 有其适用的场景，在这个云时代，它作为与 NewSQL 完全不同的思路开拓了一条满足部分云用户需求的道路，但是软件世界里面没有银弹，未来还有很多的挑战需要克服。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.79】小米开源 SQL 优化工具 SOAR 技术内幕</title>
      <link>https://pingcap.com/meetup/meetup-79-20181107/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-79-20181107/</guid>
      <description>在 Infra Meetup No.79 上，来自小米的李鹏翔老师为大家分享了小米开源的智能 SQL 优化工具——SOAR，并进行了现场 Demo 演示，以下是现场视频 &amp;amp; 文字回顾，enjoy~
在过去的几年间，小米互联网业务高速发展，数据库规模也在不断的增长。为了提供稳定、高效的数据库服务，进一步的提高 DBA 工作效率，解放生产力，小米 DBA 基于 Go 语言自主研发了智能 SQL 优化改写工具 SOAR。该工具在内部使用期间效果显著，小米运维部决定将其开源，为开源数据库生态助力。 在 10 月 20 日的开源先锋日（OSCAR）上，小米正式宣布开源自研的 SOAR（SQL Optimizer And Rewriter），开源后两周时间 GitHub 上的 Star 数便超过了 2700。
 视频 | Infra Meetup No.79：小米开源 SQL 优化工具 SOAR 技术内幕
 PPT 链接
  SOAR 是一款智能 SQL 优化和改写工具，开发人员可以直接通过 SOAR 快速的对自己的 SQL 进行质量检查，生成评估报告，防止将问题 SQL 带到线上从而导致服务质量下降。它不仅能够尽可能地提高线上代码质量，还能避免一些由于人为疏漏而带来的隐患。
在本期 Meetup 上，李鹏翔老师主要介绍了 SOAR 的基本使用场景和使用方式，介绍了在不同操作系统下如何快速上手 SOAR，并讲解了 SOAR 的常用配置，在现场 Demo 过程中对一些常见问题进行了解答。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.78】Bigdata@Bilibili &amp; Chaos Practice in TiDB</title>
      <link>https://pingcap.com/meetup/meetup-78-20181030/</link>
      <pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-78-20181030/</guid>
      <description>上周六在上海举办的 Infra Meetup No.78 上，B 站数据平台技术经理薛赵明老师和我司首架唐刘老师带来了精彩分享，以下是视频&amp;amp;文字回顾～
《B 站大数据建设实践》  视频 | Infra Meetup No.78 - 薛赵明 - B 站大数据建设实践
 PPT 下载链接
  本次分享薛赵明老师主要介绍了 B 站在大数据建设方面的历程及不同时期做的选择和其中犯的错误。主要涉及我们在存储、调度、计算、分布式队列方面的一些技术选型。
我们在离线存储上采用的还是社区的 HDFS，大数据的 KV 存储上我们尝试了 HBase、ES 等组件，同时对于业务属性分为了 online 和 offline 集群。在随着集群规模的扩大上， namenode 也遇到了不少的挑战，例如内存过大，队列过长、存储空间等方面。
调度层选择的 YARN，不过基于该组件我们在外围做了一些保障性的工作，例如队列资源的利用率，自动调整分配，作业执行成功率，提交成功率等。
薛赵明，Bilibili（哔哩哔哩）数据平台技术经理
计算层区分了批量计算（Hive,MR,Spark）、流式计算（Flink,Spark streaming）、ad-hoc（Presto）、OLAP（Kylin）。平台层提供计算方式，业务方自己选择符合合适的计算场景。
消息队列上采用的是 kafka，在 0.10.1.1 这个版本上我们遇见了不少问题，例如 conusmer log skew, produce block , multiple Kafka controllers等。经过近两年的使用，最近计划迁移到最新的 2.0 版本。
上层服务上，基于我们的大数据套件，针对不同的用户，我开发了相应的大数据工具和数据产品，例如开发 IDE，报表工具，监控系统,数据交换工具等等。
《Chaos Practice in TiDB》  视频 | Infra Meetup No.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.76】列式存储如何进行在线更新</title>
      <link>https://pingcap.com/meetup/meetup-76-20181023/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-76-20181023/</guid>
      <description>视频 | Infra Meetup No.76 - 韦万 - 列式存储如何进行在线更新 PPT 链接  时隔一月，我们又与广州的社区小伙伴们相聚啦～这次是由我司数据库核心研发工程师韦万老师带来的《列式存储如何进行在线更新》主题分享。他首先介绍了 OLAP 场景与 OLTP 的区别，以及为何列式数据库特别适合 OLAP 场景，并介绍了主流的对 OLAP 进行优化的技术。
然后进入主题，韦万老师分别列举了目前流行的几种列式数据库的更新方案，包括 SQL Server, Vertica, Kudu 以及 VectorWise， 并分析了它们的优缺点。最后介绍了同学们比较关注的部分，即 TiDB 作为一款 HTAP（Hybrid Transactional/Analytical Processing）数据库，当前的架构以及最新进展（视频中剧透了“神秘武器”——TheFlash）。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.77】我司成都分舵第一次 Meetup</title>
      <link>https://pingcap.com/meetup/meetup-77-20181023/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-77-20181023/</guid>
      <description>此次成都 Infra Meetup，恰逢我司成都 Office 正式成立，驻成都的所有 PingCAPer 及 Contributor，与到场小伙伴一起让这首场成都 Infra Meetup 充满了庆祝的热烈气氛：一场成都 TiDB 社区小伙伴的线下见面会开始了。
崔秋，我司联合创始人
在我司联合创始人崔秋做了感谢社区的开场白后，我司技术副总裁申砾老师带来《Deep Dive Into TiDB SQL Layer》的分享，他首先为大家介绍了 TiDB 的整体架构，重点分享了 SQL 层的架构和核心组件，包括 Query Optimizer 和 Execution Engine，并举例说明了其中的实现细节。最后申老师简要介绍了 TiDB 的 Roadmap，鼓励大家通过各种方式参与 TiDB 开源社区里来。（欢迎捞 issue 提 PR 成为 TiDB Contributor，我们会有神秘小礼物相送哦～）
 视频 | 申砾-Deep Dive Into TiDB SQL Layer PPT 下载链接  申砾，我司技术副总裁
茶歇时间，大家三三两两聚在一起讨论，现场 PingCAPer 很耐心的一一回答大家关于 TiDB 技术细节的问题，现场讨论氛围非常热烈。
茶歇过后，马上消费金融 NewSQL 负责人李银龙老师为大家分享了 TiDB 实践经验。
 视频 | Infra Meetup No.77 - 李银龙 - 马上消费金融 TiDB 实践分享  李银龙，马上消费金融 NewSQL 负责人</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.75】这次我们聊了聊 Google F1 的最新论文</title>
      <link>https://pingcap.com/meetup/meetup-75-20181016/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-75-20181016/</guid>
      <description>上周六我们在「新根据地」举办了第 75 期 Infra Meetup，由于是在新办公室举行的第一场 Meetup，到场的小伙伴都收到了特别福利——社区 T 恤一件～很多小伙伴还好奇参观了一圈我们的工作环境。“你们公司为什么要这么大的会议室啊？” ——要为定期举办的社区交流活动准备空间呀～不过照本期火爆程度，这个空间恐怕不久就不够用了 haha
言归正传，在本期 Meetup 上，我司 CTO 黄东旭从最新的论文出发，结合 Google F1 团队在今年 VLDB 上的演讲内容，为大家分享了 F1 Query 的架构原理。以下是文字 &amp;amp; 视频回顾，enjoy~ 视频回顾  视频 | Infra Meetup No.75 - F1 Query: Declarative Querying at Scale PPT 链接  Google 在今年的 VLDB 上发布了 F1 的新进展（《F1 Query: Declarative Querying at Scale》），距离 Google 的上一篇 F1 论文已经过去 5 年了。2013 年论文《F1: A Distributed SQL Database That Scales》中的 F1 是基于 Spanner 的，主要提供 OLTP 服务，而新的 F1 的定位则是大一统：旨在处理 OLTP/OLAP/ETL 等多种不同的 workload。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.74】TitanDB 首次公开分享</title>
      <link>https://pingcap.com/meetup/meetup-74-20180919/</link>
      <pubDate>Wed, 19 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-74-20180919/</guid>
      <description>上周六在广州举办的 Infra Meetup No.74 上，我司 TiKV 核心开发工程师黄华超老师为大家介绍了我们自研的 TitanDB——TitanDB 是基于 RocksDB 做的 key-value 分离的实现，主要解决大 value 写放大严重的问题。这次 Meetup 是 TitanDB 第一次公开分享。现场的小伙伴从 TiDB 的架构开始，由浅入深，最后对 TiKV、TitanDB 的架构都有了深入的了解，分享结束后的自由讨论依然非常热烈～ 以下是视频 &amp;amp; 文字回顾，enjoy～
 视频 | Infra Meetup No.74 - 黄华超 - TitanDB PPT 下载链接  华超老师先给大家讲解了 TiDB 和 TiKV 各自的架构，以及 TiDB 和 TiKV 的读写交互流程，并且解答了大家的一些问题。然后集中介绍了 TiKV 存储相关内容，包括 TiKV 是如何使用 RocksDB 的，使用过程中遇到的一些问题以及写放大的计算和如何在读写之间做权衡。最后介绍了 KV 分离的概念以及 Badger 和 TitanDB 的实现和优化。
TitanDB 是基于 RocksDB 做的 key-value 分离的实现，主要解决大 value 写放大严重的问题。TitanDB 通过把大的 value 从 LSM-Tree 中分离出来，减少 LSM-Tree 的写放大，但是会对读的性能造成一些影响，实际使用中需要根据业务情况选择把多大的 value 分离出来。TitanDB 可以说是给读写放大以及空间放大之间的权衡提供另外一种选择。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.73】TiKV 原理剖析</title>
      <link>https://pingcap.com/meetup/meetup-73-20180905/</link>
      <pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-73-20180905/</guid>
      <description>在上周六举办的 Infra Meetup No.73 上，我司 TiKV 核心开发工程师张金鹏老师分享了 TiKV 的原理与正在开发的新功能，整整讲满了 100 分钟～这可能是近期关于 TiKV 最深入的一次分享交流了吧~haha），结束后现场的小伙伴三五成群，意犹未尽地聚在一起讨论交流，钻研精神可嘉！以下是现场视频&amp;amp;文字回顾，enjoy～
 视频 | Infra Meetup No.74 - 张金鹏 - TiKV 原理剖析 PPT 下载链接  金鹏老师首先介绍了 TiKV 中的几个概念，包括 Region、Peer 和 ts，其中 Region 代表一段连续的数据，Peer 是 Region 的一个副本，ts 表示时间戳。然后从宏观的角度分析了数据在 TiKV 之间是如何分布的，以及如何进行 balance，并介绍了 TiKV 的分层结构，分析了读和写请求在 TiKV 内部的各个层之间是怎样流转的。
接着，他重点介绍了 TiKV 的几个核心组件，包括 Multi-raft、RocksDB、分布式事务、Coprocessor、GC 和调度。其中 Multi-raft 涉及到 region 的 split 和 merge，以及 leader lease、pre-vote、learner 等概念；RocksDB 相关的内容包括 column family、delete files in range、ingest sst files、多线程 compaction、sub-compaction 等。（课代表温馨提示：金鹏老师对这些概念的讲解非常细致深入，也耐心解答了现场小伙伴的提问， 对这些名词不太熟悉的朋友赶紧点开视频跳到相关章节观看吧！）</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.72】TiDB Operator，让 TiDB 成为真正的 Cloud-Native 数据库</title>
      <link>https://pingcap.com/meetup/meetup-72-20180820/</link>
      <pubDate>Wed, 22 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-72-20180820/</guid>
      <description>TiDB Operator 是 TiDB 在 Kubernetes 平台上的自动化部署运维工具，目前已经开源。在上周六举办的 Infra Meetup 第 72 期上，我司邓栓老师为大家分享了 TiDB Operator 开源的细节，并演示了单机快速体验 TiDB Operator 的操作。 以下是邓栓老师撰写的技术详解文章和 Meetup 现场视频。希望大家通过文字和视频深入了解 TiDB Operator 之后，可以速来贡献代码、成为 Contributor ！( ´▽｀)  TiDB Operator 是 TiDB 在 Kubernetes 平台上的自动化部署运维工具。目前，TiDB Operator 已正式开源（pingcap/tidb-operator）。借助 TiDB Operator，TiDB 可以无缝运行在公有云厂商提供的 Kubernetes 平台上，让 TiDB 成为真正的 Cloud-Native 数据库。
要了解 TiDB Operator，首先需要对 TiDB 和 Kubernetes 有一定了解，相信长期以来一直关注 TiDB 的同学可能对 TiDB 已经比较熟悉了。本文将首先简单介绍一下 TiDB 和 Kubernetes，聊一聊为什么我们要做 TiDB Operator，然后讲讲如何快速体验 TiDB Operator，以及如何参与到 TiDB Operator 项目中来成为 Contributor。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.71】TiDB 2.1 新特性与未来规划</title>
      <link>https://pingcap.com/meetup/meetup-71-20180710/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-71-20180710/</guid>
      <description>在上周六举办的 Infra Meetup No.71 上，我司申砾老师重点介绍了 TiDB 2.1 Beta 版本在 Raft / PD / SQL 执行引擎等方面的新特性以及未来的规划（中间穿插着我司 CTO 的各种「插播新闻」😂）。当天虽然下着小雨，但丝毫没有影响大家的热情，活动结束后还有不少童鞋留下来讨论哦～以下是现场视频&amp;amp;文字回顾，enjoy ！
视频回顾 视频 | Infra Meetup No.72：TiDB 2.1 新特性与未来规划
可下载 完整 PPT 配合观看
干货节选 TiDB 2.0 版本于今年 4 月底发布，经过两个月的开发，2.1-Beta 版本于 6 月底发布。这个版本在 2.0 的基础之上做了不少改进。
在 Raft 方面，2.1 最大的变化是引入了 Learner 和 PreVote 两个特性。其中 Learner 可以加强调度过程中的数据安全性，并且为将来 OLAP 请求读 Learner 副本打下基础；PreVote 可以增强系统的稳定性，降低诸如网络隔离后节点重新加入造成的系统抖动。
在 PD 方面，2.1 优化了热点调度功能，收集更详细更准确的集群负载信息，并做更合理的调度在 SQL 优化器方面对 CBO 框架做了进一步改进，提升代价估算准确度。
在 SQL 执行引擎方面，将 Hash 聚合算子以及 Projection 算子做了并行化，提升大数据量下查询的性能。同时我们也在探索 OLTP 场景下的性能提升方案，预计到 2.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.70】Paper Party：CEO 解读 TiDB 下一代存储引擎</title>
      <link>https://pingcap.com/meetup/meetup-70-20180626/</link>
      <pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-70-20180626/</guid>
      <description>上周六举办的 Infra Meetup No.70，我们换了一个开阔些的场地——嗯，没看错，是我司的一间办公室，然而掏空了房间里的椅子沙发，还是不够坐。
爆满的原因当然是我司 CEO 刘奇的「重磅分享」：刘奇分享了受威斯康辛的论文启发的 TiDB 下一代存储引擎的设计考量及实践，以及「关门福利」——非常强悍的测试结果，以下是现场视频 &amp;amp; 文字回顾，enjoy！
视频回顾 视频 | Infra Meetup No.70：CEO 解读 TiDB 下一代存储引擎
论文 slides 链接
我司 CEO 刘奇首先为大家介绍了新的磁盘进化发展趋势，如何做软硬件协同设计，以及硬件的发展对数据库系统架构的影响。
干货节选 存储引擎是数据库的核心组件之一，目前 TiDB 使用 LSM-Tree 作为底层的存储引擎，其良好的顺序写入特性得到了很大的发挥。然而 LSM-Tree 模型本身也不是尽善尽美，其中较为突出的缺点是写放大比较严重。该问题也吸引了不少学者的研究，也有不少改进论文出现。来自威斯康辛的论文 WiscKey: Separating Keys from Values in SSD-conscious Storage 是其中的典型代表。
刘奇接着介绍了新一代存储引擎利用新的硬件特性的方式（比如充分发挥 SSD/NVMe/Optane 的多通道写入对存储引擎的提升），并解读了威斯康辛的论文在这方面的实践——利用多通道的并行能力来弥补 Key-Value 分离带来的开销。这个方法实现简单，效果极佳。TiDB 的新一代存储模型也受到这篇论文的启发。
最后，刘奇分享了 PingCAP 在这方面的思考与实践，以及对下一代存储引擎设计的具体考量，并展示了正在研发的 TiDB 下一代存储引擎的强悍实测性能。测试结果显示，相比当前的版本，系统整体性能提升了 2-10 倍。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.69】CASPaxos，一个有趣的 RSM 算法</title>
      <link>https://pingcap.com/meetup/meetup-69-20180612/</link>
      <pubDate>Tue, 12 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-69-20180612/</guid>
      <description>上周六，Infra Meetup 时隔一个月终于回归北京大本营，北京的朋友们格外热情，会议室最后都挤不下啦 ～现场有几位朋友拿着提前打印的论文认真地记笔记，分享结束后大家还围绕 CASPaxos 讨论了很久，瞬间有种“Paper Party”的感觉——看来 Infra Meetup 不定期的论文分享大大激发了社区小伙伴的“学术”之心啊！（不过，全场最大的亮点还是我司 CTO 的“魔性”PPT……）
视频回顾 视频 | Infra Meetup No.69：CASPaxos，一个有趣的 RSM 算法
配合 PPT 观看更佳～
干货节选 本期 Meetup 我司 CTO 黄东旭分享了一篇有趣的论文——CASPaxos: Replicated State Machines without logs。他首先通过一个简单的例子通俗易懂地介绍了经典 Paxos 的算法。随后引入了 RSM（日志复制状态机）的概念 ， 并指出 CASPaxos 其实是在经典 Paxos 的基础上进行了拓展，变成了没有日志的 RSM 。接着，他介绍了 CASPaxos 的主体算法，包括 membership change 算法以及用 CASPaxos 实现一个通用数据库时需要考虑的问题。
来自大神的“魔性” PPT
东旭接着对比了目前常用的 RSM 算法 ，比如 TiDB 中用到的 Raft 算法与 CASPaxos 的区别。相较而言，CASPaxos 目前是一个偏学术性的理论，在工业上应用的完整度和相关优化算法还不够。CASPaxos 的优点在于出现异常时的不可用时间非常短，并且没有额外的日志开销，缺陷是做数据丢失的故障恢复代价比较高，而且读依然是多数派读，对业务上的灵活性会有一些影响。
P.S. 东旭还和现场的朋友们一起针对 CASPaxos 的缺点，大开脑洞，畅聊了一些可能的优化方法 。现场越聊越嗨，不得不说大家想法都很“清奇”啊 ( ´▽｀) 。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.68】虚怀迎远客 魔都 Talk 「长」</title>
      <link>https://pingcap.com/meetup/meetup-68-20180531/</link>
      <pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-68-20180531/</guid>
      <description>距离去年在上海举办的 TechDay 已经过去了近一年，上海社区小伙伴们积攒已久的热情终于在上周六的 Infra Meetup 现场释放了出来~ 现场爆满不说，Q&amp;amp;A 环节大家都抢着与讲师互动，结束后还有小伙伴意犹未尽，强烈要求多在上海举办这样的技术交流趴（我们会努力的，嗯💪）。以下是现场视频&amp;amp;文字回顾，enjoy！
现场同学坐定之后，我司 CTO 黄东旭简短开场，欢迎 Percona CEO Peter Zaitsev 做客 Infra Meetup No.68 上海站～随后 Peter 带来了 Using MySQL for Distributed Database Architectures 的主题演讲。
Using MySQL for Distributed Database Architectures Percona CEO Peter Zaitsev
视频链接：1st Talk by Peter Zaitsev
Peter 首先介绍了 MySQL 最近几个版本迭代的性能升级数据。他认为 MySQL 单机性能提升很大，但是与分布式数据库在应用场景上仍有较大的区别，很多问题是单机解决不了的。从而引出了基于 MySQL 做分布式尝试的方法论，并从高可用、扩展性、数据分布策略几个方面进行了详细解读。随后分享了在 MySQL 上实现分布式计算和分布式系统的方法。
Peter 提到不同业务对数据库的需求不一样，对隔离级别和一致性的要求也不一样, 需要仔细思考相关配置。他重点介绍了 Percona 数据库管理工具和集群方案，同时分享了对市面上常见的基于 MySQL 分片的中间件的看法，以及对 TiDB 等 NewSQL 未来发展的期待。
How to build a Self-Driving database PingCAP CTO 黄东旭</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.67】杭州站</title>
      <link>https://pingcap.com/meetup/meetup-67-20180509/</link>
      <pubDate>Wed, 09 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-67-20180509/</guid>
      <description>上周日 Infra Meetup 首次走进杭州，感谢热情的杭州社区小伙伴们冒雨参加活动～这次活动由我司数据库专家马晓宇老师和资深数据库架构师房晓乐老师为大家带来精彩的分享，还有来自二维火、挖财、蘑菇街的社区小伙伴带来了三个闪电 Talk，分享了他们的 TiDB 实践经验。
马晓宇：TiDB 架构及 2.0 详解 首先，我司数据库专家马晓宇老师带来《TiDB 架构及 2.0 详解》精彩分享，介绍了 TiDB 的方方面面，包括存储模块 TiKV，调度模块 PD，计算模块 TiDB， OLAP 组件 TiSpark，数据流转 Syncer 和 Binlog 以及 Cloud 集成等等众多组件和独立产品。
马晓宇老师更与现场的同学深入探讨了 TiDB 背后的技术细节以及一些重大选择的原因。
例如，从单体 KV 演进到分布式，就不得不思考如何多副本容错，且还需要在多个副本之间达到一致性，这是选择 Raft 协议的根本动机。然而，仅仅是 Raft 并不能满足扩展性的需求，因此又引入了 Raft Group / Region 的分片机制，再加上 PD 模块的调度，让数据和负载均衡得以实现。
在 TiDB 部分，马晓宇老师重点介绍了数据库计算引擎的详细架构，包含模块及其不同作用，详细讲解了用户的 SQL 输入如何被分解分析之后产生执行计划并执行，以及数据在 TiDB 中如何将行数据以及索引编码成 TiKV 所需的键值对等等。
另外，4 月底 TiDB 2.0 GA 和 TiSpark 1.0 正式发布，马晓宇老师在分享中也提到了 TiDB 和 TiSpark 版本的重大提升：相对 1.0 版本，TiDB 加强了稳定性和性能，针对 TPC-H 等分析型场景，TiDB 有了本质的飞跃。而 TiSpark 1.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.66】Application of TLA&#43; at PingCAP</title>
      <link>https://pingcap.com/meetup/meetup-66-20180417/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-66-20180417/</guid>
      <description>上周六的 Meetup 上，我司董麒麟同学为大家讲解了 TLA+ 在 TiDB 中的应用。现奉上现场视频 &amp;amp; 干货节选，Enjoy～
视频回顾 视频 | Infra Meetup No.66：Application of TLA+ at PingCAP
可下载 完整 PPT 配合观看
干货节选 TLA+ 是一个用来设计、描述和验证并发系统的一套形式化语言，易学易用。在 TiDB 中，我们非常关心一些关键系统的设计正确性，所以使用 TLA+ 来保证这一点。PingCAP 在 2017 年底开始尝试使用 TLA+，到目前为止，已经用 TLA+ 验证了我们优化过的 Percolator 协议以及 Multi-raft region merge 的正确性。这些代码可以在 pingcap/tla-plus 上找到。
TLA+ 可以在一个比代码更高的层面上描述系统。在编写代码之前，将系统完整地表述一遍是很重要的。这能强迫我们去思考这个系统的细节，避免早期设计时出现失误并保证正确性。TLA+ 的基本原理是将系统描述成为一个状态机。系统可以用 TLA+ 来抽象出若干变量表达它的当前状态，并用形式化的语言去描述这个状态机的初始结束状态与状态转移。我们对这个系统的一些 Safety 性质比较感兴趣，这些 Safety 的性质也可以在 TLA+ 中用谓词来刻画。另外一个工具 TLC 可以用来验证被 TLA+ 抽象出来的系统模型。TLC 的原理是枚举状态机的所有可以遍历到的状态集。验证系统的正确性就是确保所有状态都满足对应的谓词。
TLA+ 在 TiDB 的第一个应用是 Percolator 事务协议。这个协议是一个二阶段提交算法，用来在只支持单行事物的存储上实现多行事务。这个协议的具体介绍可以在 这里 找到。和原始协议的不同，TiDB 做了一个很重要的优化，在 prewrite 阶段，我们并不是采取了先 prewrite 主锁，再并发副锁的策略，而是主锁和副锁一起并发。但是如果直接这样设计是存在问题的。在视频中可以看到我们是如何用 TLA+ 定位这个问题，然后提出了一个解决方案来克服这个问题。我们用 TLA+ 验证了优化的正确性。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.65】「四美具，二难并」之成都行</title>
      <link>https://pingcap.com/meetup/meetup-65-20180404/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-65-20180404/</guid>
      <description>上周 Infra Meetup 走进了成都，来自 G7 汇通天下的廖强老师和来自 PingCAP 的申砾、孙浩老师，为大家带来了三个干货满满的 Talk 。 这是第二场走出帝都的 Meetup，场面依然火爆～「四美具，二难并」 成都，唯有美食与同道者不可辜负！
下午两点大家陆续进场，不得不说成都的同学们太热情了，场地差点坐不下，各式各样的椅子都被搬来了～等同学们坐定之后，PingCAP Engineering VP 申砾老师首先上台，深入讲解了 TiDB 的各项核心性能，让同学们对 TiDB 的架构和性能有了充分的认知。
申砾：《Deep Dive into TiDB》 视频回顾 | Infra Meetup No.65 成都站：Deep Dive into TiDB（申砾）
 PingCAP Engineering VP 申砾
 申砾老师从系统整体到技术细节，从核心项目到周边工具，介绍了 TiDB 的方方面面。
 TiDB 的设计目标、核心特性以及整体架构。
 系统分层介绍，包括分布式 Key-Value 存储引擎 TiKV 的核心技术及实现细节，分布式 SQL 引擎的设计思路以及优化器、执行引擎等核心组件的介绍。
 Cloud TiDB、TiSpark 等核心项目以及 TiDB 集群的周边工具。
 Q&amp;amp;A 环节：TiDB 在实践中的使用经验，TiDB 2.0 版本的最新进展以及如何实现性能上的巨大提升。
  短暂休息之后，G7 汇通天下技术合伙人廖强老师讲述了他为什么选择了 TiDB ，以及 G7 的实践方案。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.64】Chaos Practice in TiDB</title>
      <link>https://pingcap.com/meetup/meetup-64-20180314/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-64-20180314/</guid>
      <description>上周六的 Meetup 上，我司首席架构师唐刘为大家分享了《Chaos Practice in TiDB》。现场小伙伴们热情十足，于是就有了这场“分享 40 分钟，畅聊 1 个半小时 ”的技术趴~ :-D 附带唐刘老师撰写的《混沌工程简介》一文，希望可以帮助未到现场的小伙伴们更好地了解本期 talk :)
视频回顾  视频 | Infra Meetup No.64 : Chaos Practice in TiDB PPT 链接  干货节选 构造一个健壮的分布式数据库系统是一件非常困难的事情，因为我们需要做非常多的工作来保证用户数据安全，不允许数据丢失或者损坏。而在 TiDB 里面，我们是通过实践 Chaos Engineering 来保证。
在本次分享中，我司首席架构师唐刘首先提出了 Chaos 测试的必要性：“虽然我们有 unit test，integration test 这些，但他们都是有局限性的，为了更好的模拟系统实际的情况，我们需要 Chaos。”
那么在 TiDB 里面是如何做 Chaos 的。在这其中有三个关键技术，monitor，fault injection 以及 automation。现场重点讲解了 fault injection，包括进程干扰，网络干扰，文件干扰等，以及一些集群工具。同时介绍了在 TiDB 里面如何将所有这些进行整合，也就是 Schrodinger 平台，通过 Schrodinger，我们能自动化的进行 Chaos test。
最后，唐刘老师还为大家介绍了一些 PingCAP 现在的研究方向，譬如使用 TLA+ 来证明我们程序的正确性，以及使用 automating fault injection 来自动的分析系统，进行 fault injection。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.63】深入了解 TiDB 新执行框架</title>
      <link>https://pingcap.com/meetup/meetup-63-20180207/</link>
      <pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-63-20180207/</guid>
      <description>在上周六的 Meetup 中，我司 TiDB SQL 组研发工程师张建同学，为大家解读了 TiDB 的新执行框架。农历新年前的最后一期 Meetup，也是满满干货哦～
视频回顾  视频 | Infra Meetup No.63：深入了解 TiDB 新执行框架 PPT 链接  干货节选 在 TPC-H 1G 的性能测试结果上，TiDB release 1.1 相比于 1.0 有着巨大的性能提升，获得这些性能提升最主要的两个优化是：
 执行器框架由传统的 volcano 模型一次 next 函数调用返回一行结果变为了返回一批结果，减少了框架上的函数调用开销并且给予了每个物理算子更多的优化空间；
 这一批结果采用紧凑的、列式的内存布局，使得执行器在计算的时候更加缓存友好。
  以 Table Reader 为例，使用新的执行框架和内存布局以后，通过减少函数调用开销以及已申请内存的重复利用，读取 600 万行数据的速度提升了 30% 左右。
本次分享中，张建同学从表达式计算入手跟大家对比了两种表达式计算方式的性能差异以及原因。
第一种计算方式是以行为单位，用一条输入数据计算出一条输出数据，这种计算方式是 1.0 版本采用的；
第二种计算方式是表达式为单位，先计算第一个表达式得到输出的第一列，然后计算其他表达式得到其他输出列，这是目前 1.1 所采用的计算方式；
我们通过简单的 benchmark 证明第二种计算方式快 15% 以上，并通过采集 L2 Cache performance 推断出了第二种计算方式的 L1 Cache Miss 的确比第一种少从而证实了这些性能差距跟 L1 Cache Miss 正相关。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.62】Apache Pulsar</title>
      <link>https://pingcap.com/meetup/meetup-62-20180110/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-62-20180110/</guid>
      <description>在上周六的 Meetup 中，我们邀请到了来自 Streamlio 的翟佳老师，为我们分享了《Unified Streaming and Queuing with Apache Pulsar》
 视频 | Infra Meetup No.62：Apache Pulsar
 PPT 链接
  Apache Pulsar 是 Yahoo 开源的下一代的消息系统，Pulsar 于 2016 年底开源，现在是 Apache 软件基金会的一个孵化器项目。 Pulsar 在保证大数据消息系统的性能和吞吐量的同时，提供了更多企业级的 Feature，包括方便的运维和扩展，灵活的消息模型，多语言 API，多租户，异地多备，和强持久性一致性等等，解决了现有开源消息系统的一些不足。 Pulsar 在 2015 年初被大规模部署在 Yahoo 的生产环境中，支持着公司的主要应用和业务，为 Yahoo 全球 10 个数据中心之间提供了数据全互备；支持着 140 多万个 Topic；每天处理 1000 多亿条消息；整体的消息发布延迟小于 5ms。通过了 Yahoo 内部主要应用（比如广告平台，KV 系统，mail 等）生产环境的检验。 Pulsar 在设计之初就对企业级应用中比较关注的多租户和异地多备等 feature 进行了全面的支持； Pulsar 对消息系统中比较难解决的强一致性和持久性问题给出了比较优雅的解决方式。 这次分享中，翟佳老师介绍了 Pulsar 项目产生的背景，Pulsar 中的相关概念，Pulsar 主要的体系架构，Pulsar 的特性，Pulsar 在设计中的考虑和具体实现。最后对比了和 Kafka 在同一环境下的相关测试结果。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.60】初探 Orca 查询优化器</title>
      <link>https://pingcap.com/meetup/meetup-60-20171223/</link>
      <pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-60-20171223/</guid>
      <description>上周六，PingCAP Infra Meetup 迎来了第 60 期 👏 由我司 “SQL 小王子”韩飞同学出台，为大家带来了《初探 Orca 查询优化器》主题分享~
视频回顾 视频 | Infra Meetup No.60：初探 Orca 查询优化器
可下载 完整 PPT 配合观看
干货节选 Orca 优化器是基于代价面向 MPP 执行引擎的优化器，使用了先进 Cascades 模型，将优化分为 Exploration，Stats Derivation，Implemetation 等阶段。Orca 优化器可以将优化任务分解，利用多核 CPU 并行执行，以加快优化速度。
知乎上有个热门问题：在做一个数据库的过程中，最难的是哪个部分？
很多人都认为查询优化器可能是数据库中一个最难的部分。也有人会有疑问：一个 SQL 生成一个执行计划可能是一个很确定的事情，为什么会是最难的？
对此，韩飞同学表示，难点主要集中在基于代价的物理计划生成。
在本次分享中，韩飞同学从逻辑计划的优化及物理计划的优化讲起，重点介绍了 Orca 优化器的架构，算法实现，优化效果以及测试保证等问题。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.58】TiSpark 架构设计与实现</title>
      <link>https://pingcap.com/meetup/meetup-58-20171115/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-58-20171115/</guid>
      <description> 在上周六的 Meetup 中，我司 TiSpark 项目负责人马晓宇同学，与大家分享了《TiDB 遇到 Spark-TiSpark 架构设计与实现》。
TiSpark 是一款直接运行在分布式数据库 TiDB 存储层的产品，这样紧密结合的设计使它能够提供传统的 Spark SQL 所没有的诸多特性。TiSpark 与 TiDB 在同一套存储上共同支持了分析和在线事物处理两种场景，让复杂的数据平台架构变的简单，减少复杂的 ETL 流程，让大数据分析变的更实时。本次分享，马晓宇同学与大家聊了聊 TiSpark 的架构和实现细节。这一聊，就聊成了一场讨论时长大于分享时长的趴 :-D
视频回顾  视频 | Infra Meetup No.58：TiSpark 架构设计与实现
 PPT 下载链接
  PPT节选 </description>
    </item>
    
    <item>
      <title>【Infra Meetup No.57 Rust 专场】Rocket Web 框架解析</title>
      <link>https://pingcap.com/meetup/meetup-57-20171025/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-57-20171025/</guid>
      <description>Rust 专场 2.0 Rust 作为系统级编程语言，同样可以非常方便的开发上层 Web 应用。借助目前 Rust 社区最火的 web 框架 Rocket，可以像动态语言一样方便地创建高性能的 Web 应用，同时可以拥有 Rust 强大的类型安全保障。
在上周六，我们邀请了 Rocket 的作者 Sergio Benitez，与大家面对面分享了《Rocket Web 框架解析》。
据 Sergio 现场表示，这是他首次来中国，以往虽然也有在公开场合解读过 Rocket Web 框架，但本次，有些新鲜内容可是第一时间共享给 Rust 中国社区的小伙伴哦~
这一次，让我们跳过现场内容解读环节，直接为大家奉上新鲜出炉的干货视频，enjoy~~
视频 | Infra Meetup No.55：Rocket Web 框架解析
讲师介绍： Sergio Benitez，斯坦福大学博士四年级的学生，主要研究如何将编程语言理论与操作系统和安全性融合在一起。目前在做项目包括对 Rust 的类型系统 “Rusty Types” 的规范化，以及 Rust 的 Rocket Web Framework。在斯坦福大学之前，Sergio 曾在 Google、Apple 和 SpaceX 实习，参与的项目包括设计异常检测算法，火箭及其它航天器的操作系统的性能调优。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.56】MonetDB/X100 Paper 解读</title>
      <link>https://pingcap.com/meetup/meetup-56-20170920/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-56-20170920/</guid>
      <description>上周六，PingCAP Infra Meetup 第 56 期特设论文专场，我司核心工程师张建与大家一起分享并解读了“MonetDB/X100: Hyper-Pipelining Query Execution” 论文。此篇论文作为分析型数据库领域内引用次数最多的论文之一，它为何如此火爆？在今天的文章里你应该可以找到答案。
精彩视频 视频 | Infra Meetup No.56: MonetDB/X100 Paper 解读
精彩现场 在 PingCAP Infra Meetup 第 56 期论文专场，来了很多对 MonetDB/X100 论文感兴趣的小伙伴们。分享一开始，我司联合创始人兼 CEO 刘奇就为何选择 MonetDB/X100 这篇论文分享了自己看法。
刘奇提到:&amp;ldquo;如果大家有阅读近两年新出的一些 Paper，会发现里面引用率最高的一篇文章就是 MonetDB/X100。MonetDB/X100 发表于 2005 年，其实不算新。但读过该论文的人会发现目前主流的 OLAP 系统相关的技术，基本上都能在这篇论文中找到影子，如文中提到了列存、Pipeline，甚至是 JIT。他做 JIT 的思路不一样，都是比较早就有的，所以这是一篇很不错的论文。现在也可以看到很多性能比较的时候，大家新做了一个系统，说我的性能非常好，会拿出来 benchmark 说你看我打败了 MonetDB。
另外还有一些比较创新的项目，多是基于 MonetDB 改造的。一个就是英特尔最近出的一篇论文，他把 MonetDB 改造一下，把正则表达式的搜索，放到 FPGA 里面去。英特尔最近出了一款服务器，这个服务器的 CPU 和 FPGA 是放在一起的，他们得到 Performance 最小提倡是 2.3 倍以上，大概意思上就是说，MonetDB 在这上面做一个简单的改造，就可以适应到更新的硬件。
在 2012 年的时候，第一个提供论文、代码的基于 MonetDB 的 GPU 的 Database 也出来了。当时是在 TPCH 的 query 里面，有一些复杂的 query，提升是非常的明显。所以大家可以看到，基于 MonetDB 改造的，在 FPGA 或者 GPU上运行的系统都有，实际上这是一个非常优秀的学术的原形，今年得了十年最佳论文奖。&amp;rdquo;</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.55】TiDB Pre-GA 版本新特性介绍以及后续功能展望</title>
      <link>https://pingcap.com/meetup/meetup-55-20170906/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-55-20170906/</guid>
      <description>上周六，PingCAP Infra Meetup 第 55 期，由我司 Engineering VP 申砾为大家分享《TiDB Pre-GA 版本新特性介绍以及后续功能展望》。在活动现场，小伙伴们就 TiDB 新特性提出了很多问题，申砾在现场与大家有一番深度的交流与讨论。精彩现场小编立马为你呈现。
精彩视频 视频 | Infra Meetup No.55：TiDB Pre-GA 版本新特性介绍以及后续功能展望
精彩现场 上周，TiDB 正式发布了 Pre-GA 版本。针对 Pre-GA 版本的新特性，PingCAP Infra Meetup 第 55 期特设定 Pre-GA 详解专场。活动当天，现场来了很多关注 TiDB 的粉丝们。
简单开场后，我司 Engineering VP 申砾同学介绍到本期内容主要围绕新版本带来的变化和内部实现细节，以及这种新型的 HTAP 数据库解决的实际问题和典型应用场景等做深度解析。
技术干货节选 TiDB Pre-GA 版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能方面做了大量优化工作。本次分享中，申砾就各个组件的优化做了详解：
TiDB 在优化器方面
 RC4 已经从一个假的基于代价产品模型，切换成一个真的基于代价产品模型，也真的是用统计信息去算。在 RC3 版本中，一些代价实际上是有规则算法的，比如说，A 等于 10 设置一个过滤比例，A 大于 10 又算另外一个过滤比例，这都是一些规则，RC4 是基于代价的一个传统模型。 Pre-GA 新特性也主要对代价模型做了一些调整。
 其次，在索引选择上做了优化，可以支持不同类型字段比较的索引选择，这一优化用户反馈查询速度明显变快。
 再者，支持 Join Reorder，对于 OLTP 层面来说，Join Reorder 不太会用到，但对于一些比较复杂的场景，比如说有的用户使用参报表。这个时候有可能会出现 Join 报表，特别是在 TCH 里面，多表 Join 比较常见。下一步计划也将统计信息导入 TiSpark 里，指导 TiSpark 做 Join Reorder 。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.54】数据库计算存储分离架构分析</title>
      <link>https://pingcap.com/meetup/meetup-54-20170825/</link>
      <pubDate>Fri, 25 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-54-20170825/</guid>
      <description>上周六，PingCAP Infra Meetup 第 54 期，我们邀请到了知乎大 V 李凯（知乎 ID：郁白）为大家分享了《数据库计算存储分离架构分析》。在活动现场，郁白老师跟小伙伴们有一番深度的交流与思想碰撞。长话短说，小编带你一起回顾精彩现场。
精彩视频 视频 | Infra Meetup No.54：数据库计算存储分离架构分析
精彩现场 PingCAP Infra Meetup 第 54 期的活动现场十分火爆，活动签到时间未开始，小伙伴们就早早来到现场占位置，我想说早来的小伙伴们还是很明智的。因为&amp;hellip;&amp;hellip;
后续到场的小伙伴只能酱婶儿滴扎堆在门口竖起耳朵听了，这场活动简直是一场郁白大神与粉丝的见面会。
说了这么多，先上一张郁白老师的图吧~ 🙂
技术干货节选 大数据下公有云面临的 5 个挑战 谈到存储架构分离，为什么现在会有 Aurora 架构？包括前一阵阿里的 PolarDB 推出来以后，他们也在分析为什么要做这个东西。
郁白老师认为单就公有云来说，现在云数据面临的挑战有以下 5 个：
 跨 AZ 的可用性与数据安全性。 现在都提多 AZ 部署，亚马逊在全球有 40 多个 AZ， 16 个 Region，基本上每一个 Region 之内的那些关键服务都是跨 3 个 AZ。你要考虑整个 AZ 意外宕机或者计划内维护要怎么处理，数据迁移恢复速度怎么样。以传统的 MySQL 为例，比如说一个机器坏了，可能这个机器上存了几十 T、上百 T 的数据，那么即使在万兆网卡的情况下，也要拷个几分钟或者几十分钟都有可能。那么有没有可能加快这个速度。 还有一个就是服务恢复的速度。可能大家广为诟病就是基于 MySQL Binlog 复制。在主机压力非常大的情况下，是有可能在切换到备机以后，这个备机恢复可能需要几分钟甚至几十分钟。关键因素是回放 Binlog 的效率，MySQL 即使最新版本也只能做到 Group Commit 内的并发回放。这是数据库 RTO 指标，能不能在秒级、分钟级把这个服务恢复起来，这是一个在设计系统的时候要考虑的关键问题。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.53】知乎数据平台实践</title>
      <link>https://pingcap.com/meetup/meetup-53-20170805/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-53-20170805/</guid>
      <description>今天的 Meetup，我们邀请到了知乎数据平台负责人王雨舟为大家做《知乎数据平台实践》的技术分享。
又是一个美好的周末，勤劳的小蜜蜂们早早出来参加活动了~🙂 今天的活动现场又是爆满~ 感觉要换地儿的节奏啊~
今天 Meetup 的开场，我司联合创始人兼 CTO 黄东旭同学首先为大家分享了 TiDB 项目的最新进展。黄东旭同学好开心的样子，因为就在昨天，TiDB 正式发布 RC4 版 。
开场过后，接下来由知乎数据平台负责人王雨舟（江湖人称宇宙哥）开始为大家做技术分享。
宇宙哥真是 PingCAP 的真爱粉儿~ 穿着我司的文化衫亮相活动现场，超级有气场~
以下是部分技术干货分享，Enjoy~
宇宙哥在演讲开始先介绍了知乎大数据平台的整体架构情况
并讲解了埋点流程及使用 Protobuf 做埋点标准化规范
除此之外，宇宙哥还从以下几点来分析介绍 Druid
在知乎的实践：
 自定义多维分析功能和留存分析功能；
 如何做到实时数据分析；
 自定义指标、维度、报表、文件夹、Dashboard。
  这张 PPT 中有眼熟的部分哦😏宇宙哥用“丝般顺滑”总结了自己现在使用 TiDB 的感受，并表达了对 TiSpark 的期待✌️分享结束后，显然大家都还没有尽兴，接下来是一段时长堪比分享环节的 QA。激烈的讨论后现场小伙伴跟宇宙哥都嗨了，还没有嗨够的小伙伴我们下次见~</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.52】TiDB 自动化运维管理 —— TiDB-Operator</title>
      <link>https://pingcap.com/meetup/meetup-52-20170722/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-52-20170722/</guid>
      <description>今天的 Meetup，由我司技术大拿邓栓同学为大家分享《TiDB 自动化运维管理 —— TiDB-Operator》。
今日的帝都带着一丝凉爽，如此好天气怎能辜负。小伙伴们一清早就来到互动现场，一起来吃“营养早午餐”。
我司技术大拿邓栓同学激情满满的开始为大家做主题分享，主要从 TiDB-Operator 的功能介绍、整体架构、实现细节这几个纬度切入。
邓栓同学开场介绍到：分布式系统由于自身的复杂性，其管理和运维通常是非常困难的事情，借助 TiDB-Operator 我们能够轻松地将 TiDB 集群部署到 Kubernetes 集群之上，并做到自动化运维管理，极大地降低了人力运维成本，现场小伙伴们听呆了～
咦？what&amp;rsquo;wrong ? 黑灯瞎火嘛呢？
其实是小伙伴们在一起很专注的看 demo 演示~
活动最后，邓栓同学通过 demo 演示了 TiDB-operator bootstrap 一套完整的 TiDB 集群，然后在集群上面执行一个简单的操作就可以轻松实现扩容缩容，并且模拟物理节点挂掉时 TiDB-operator 对集群做自动恢复等各种自动化运维操作流程。
以上为最新前方报道～ enjoy 😁
讲师介绍：邓栓，PingCAP SRE 工程师，Kubernetes 爱好者，目前主要负责 TiDB 与各种云平台整合。Rust 中国社区联合创始人。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.51】百度统一分布式计算框架 Bigflow (内附 PPT 下载链接)</title>
      <link>https://pingcap.com/meetup/meetup-51-20170701/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-51-20170701/</guid>
      <description>今天的 Meetup，我们邀请到了滴滴地图事业部专家工程师王聪老师，为大家分享《百度统一分布式计算框架 Bigflow 》。
讲师介绍：王聪，滴滴地图事业部专家工程师，前百度基础架构部工程师，主要工作方向为分布式计算与流式计算，在百度负责计算表示层 Bigflow 与流式计算引擎 Flink。
活动现场听得很专注的小伙伴们，桑拿天也阻止不了大家的学习热情。
王聪老师首先展示了分布式计算在百度的发展例程，他介绍百度在 2003 年建立了自己的分布式搜索系统。08 年引入 hadoop，09 年底搭建了大规模的机器学习平台，当时用的是 MPI。10 年百度自研了两套流式计算引擎，主要用来完成点击流与展现流的 join。
基于多引擎并存、跨引擎成本高、升级困难这几个痛点，最终开发了一款叫做 Bigflow 的计算框架，Bigflow 希望用户使用我们提供的 API 写代码，Bigflow 将作业进行计划的优化和翻译，并提交到计算引擎之上。对于这样的思路，有一种说法“计算机领域的任何问题，都可以通过增加一个额外的中间层来解决”。在这里 Bigflow 就是架在用户与引擎之间的中间层。
以下是新鲜出炉的 PPT 节选，尽情享用~
附：完整 PPT 链接</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.49】TiDB Best Practice</title>
      <link>https://pingcap.com/meetup/meetup-49-20170603/</link>
      <pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-49-20170603/</guid>
      <description>今日的 Meetup，我司 Engineering VP 申砾同学亲自上阵，为大家分享了《TiDB Best Practice 》，好多使用经验及背后技术实现原理都是首次揭秘（当然，包括彩蛋）。
本期讲师：申砾，PingCAP Engineering VP，前网易有道词典服务器端核心开发，前奇虎 360 新闻推荐系统 / 地图基础数据与检索系统 Tech Lead。
TiDB 是一个分布式数据库，支持 MySQL 协议以及语法，在一些场景中都可以无缝替换 MySQL，以获得分布式的好处。但是分布式数据库有其自身的特点，想要在业务中用好需要遵循一些实践原则。
本次分享申砾同学首先介绍了 TiDB 的一些关键部分的实现原理，理解这些内部实现有利于理解 TiDB 的外在表现。然后与大家讨论了应用数据库时的典型操作的最佳实践以及要注意的事项，并对 TiDB 的适用场景进行了讲解。
PPT 很干，一点水都挤不出来&amp;hellip;随便放几张你们感受下┑(￣Д ￣)┍
最后，申砾同学还分享了 TiDB 最近的一些项目进展，并首次公开披露 PingCAP 最新动向：独立研发的 TiDB 专用的 Spark Connector 即将上线。
Spark 是当下最流行的大数据分析系统，拥有活跃的社区。PingCAP 希望能够将 TiDB 与 Spark 相结合，通过 Spark 对 TiDB 中存储的数据做实时分析，以融入这个生态。为了保证这个连接过程尽可能的高效，所以除了基本的 JDBC Connector 之外，便有了 TiDB 专用的 Spark Connector。
附：完整 PPT 链接
彩蛋来啦 视频：Demo of Spark on TiDB</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.45】Rust in TiKV</title>
      <link>https://pingcap.com/meetup/recording/rust-in-tikv/</link>
      <pubDate>Wed, 31 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/rust-in-tikv/</guid>
      <description>本文整理自 2017 年 4 月 16 日 Rust 专场 Meetup 上，我司首席架构师唐刘同学的现场分享，共享给大家。enjoy~
 Hello everyone, today I will talk about how we use Rust in TiKV.
Before we begin, let me introduce myself. My name is TangLiu, the Chief Architect of PingCAP. Before I joined PingCAP, I had worked at Kingsoft and Tencent. I love open source and have developed some projects like LedisDB, go-mysql, etc…
At first, I will explain the reason why we chose Rust to develop TiKV, then show you the architecture of TiKV briefly and the key technologies.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.48】分布式对象存储面临的挑战</title>
      <link>https://pingcap.com/meetup/meetup-48-20170513/</link>
      <pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-48-20170513/</guid>
      <description>今天的 Meetup，我们请到了来自白山云的张炎泼老师，为大家分享《分布式对象存储面临的挑战》。
本期讲师：张炎泼 (xp)，30 年软件开发经验，物理系背叛者，设计师眼中的美工，bug maker，vim 死饭，悬疑片脑残粉。曾就职新浪，美团。现在白山云，不是白云山。
在本次分享中，张炎泼老师从：海量小文件如何存储、如何节省存储成本、如何实现数据的自动恢复，三个方面，为大家进行了详细讲解。
以下是本期 PPT 节选
附：完整 PPT 下载链接</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.47】分布式定时任务中间件架构 Elastic-Job 的两种实现</title>
      <link>https://pingcap.com/meetup/meetup-47-20170506/</link>
      <pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-47-20170506/</guid>
      <description>今天的 Meetup ，我们请到了当当架构部负责人张亮，大家分享了《分布式定时任务中间件架构 Elastic-Job 的两种实现》。
 本期讲师：张亮
当当架构部负责人，主要负责分布式中间件以及私有云平台的搭建。致力于开源，目前主导两个开源项目 elastic-job 和 sharding-jdbc。擅长以 java 为主分布式架构以及以 Mesos 为主的云平台方向，推崇优雅代码，对如何写出具有展现力的代码有较多研究。
今日帝都依然大风，但小伙伴们学习的热情丝毫未减哦~
在本次分享中，张亮老师从分布式定时任务中间件的适用场景，轻量级去中心化架构方案以及基于 Mesos 的中心化架构方案，三个方面为大家进行了详细讲解。
在互联网应用中，各式各样的定时任务存于系统的各个角落，我们希望由一个平台统一将这些作业管理起来。然而，一旦平台中运行大量的作业，发现异常作业并手动处理难免会感到繁琐，同时人工处理还会带来很多其他的额外成本。如何最大限度的减少人工干预？
高可用可以让作业在被系统发现宕机之后能自动切换。而弹性化可以认为是高可用的进阶版本，在高可用的同时还能够提升效率和充分利用资源。对于动态的扩容和缩容，通常采用分片的方式实现。
去中心化架构是指所有的作业节点都是对等的，优点是轻量级，部署成本低；缺点则是，如果各作业服务器时钟不一致会产生同一作业的不同分片运行有先有后，缺乏统一调度，并且不能跨语言。
中心化架构将系统分为调度节点和执行节点，可以解决服务器时间差以及跨语言的问题；缺点是部署和运维稍复杂。
Elastic-Job 最初的版本分离于当当内部的应用框架 ddframe，是一个纯 Java 实现的分布式方案，参照 dubbo 的方式，提供无中心化解决方案。
如今，Elastic-Job 已开源近 2 年，截止目前已更新发布18 次，GitHub Star 数近 2000，成绩出色。更有多个开源产品衍生自 Elastic-Job。
应小伙伴们的强烈要求，张亮老师临时加场 Demo 演示。
最后，还有超多第一手爆料，是属于现场听讲小伙伴们的专属福利 ✌️ 很心动？下周六，老时间，老地点，PingCAP 第 48 期 Infra Meetup 等你呦！</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.46】MySQL 5.7 的特性及实践</title>
      <link>https://pingcap.com/meetup/meetup-46-20170422/</link>
      <pubDate>Sat, 22 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-46-20170422/</guid>
      <description>今天的 Meetup，我们邀请到了熊猫直播 DBA 杨尚刚老师，为大家分享《MySQL 5.7 的特性及实践》~
  讲师介绍：杨尚刚，熊猫直播高级 DBA，负责后端数据库平台建设和架构设计。前新浪高级数据库工程师，负责新浪微博核心数据库架构改造优化，以及数据库相关的服务器存储选型设计。
 2015 年最重磅的当属 MySQL 5.7 GA 的发布，号称 160 万只读 QPS，大有赶超 NoSQ L趋势。
不过官方的硬件测试环境是很高的，所以这个 160 万 QPS 对于大家测试来说，可能还比较遥远，所以实际测试的结果可能会失望。但是，至少我们看到了基于同样测试环境，MySQL 5.7 在性能上的改进，对于多核利用的改善。
本次分享中，杨老师讲解了 MySQL 5.7 在运维、优化器 Server 层、InnoDB 层等方面的优化，以及 MySQL 未来的发展趋势。
运维方面  动态修改 Buffer Pool
 MySQL redo log大小
 innodb_file_per_table
 query cache
 SQL_Mode
 binlog_rows_query_log_events
 max_execution_time
 replication info in tables
 innodb_numa_interleave
 动态修改 replication filter</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.45】Rust 专场</title>
      <link>https://pingcap.com/meetup/meetup-45-20170416/</link>
      <pubDate>Sun, 16 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-45-20170416/</guid>
      <description>今天，小伙伴们期待已久的北京 Rust Meetup 终于和大家见面啦！在这场 Rust 社区在中国的首次官方活动中，我们邀请了两位 Rust 团队核心成员，Alex Crichton、Brian Anderson，与我司首席架构师唐刘，共同为大家带来了干货十足的分享内容~第一手现场资料，看这里！
 这一次的 Meetup，小伙伴们都好积极 👏 提前一小时就有入场抢座位的~也是让小编感动到不行~~知道大家都已经迫不及待了，简单的开场之后，我们直接上干货！
Concurrency and asynchronous IO in Rust 并发在当今编程领域是如此重要，然而要想实现并发程序通常会面临数据竞争，竞态，死锁，悬空指针，多次 free 等问题，Alex 在本期 meetup 里给我们讲解了 Rust 是如何用 ownership/borrowing 系统解决这些问题的，其核心思想是:
 A mutable reference cannot be aliased
 A reference cannot outlive its referent
   Alex Crichton，Mozilla 工程师，Rust 核心团队成员。从事 Rust 编程语言方面的工作已有 5 年。在 Mozilla 主要负责 Rust 的标准库、Cargo、异步 I/O 子系统以及 Rust 本身的基础设施。目前在做异步 I/O 栈 Tokio。
 Rust 语言本身只提供了 ownership/borrowing 机制，标准库里提供了 thread，channel，arc，atomic，mutex 等基础设施，但没有提供轻量级协程，这样虽说并发程序写起来比较麻烦，但是不会只局限于一种并发模型。而后，简单介绍了第三方并行库 rayon 和 crossbeam。最后重点讲解了异步 IO 库 futures。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.44】Elasticsearch 运维</title>
      <link>https://pingcap.com/meetup/meetup-44-20170408/</link>
      <pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-44-20170408/</guid>
      <description>今天的 Meetup，我们邀请到去哪儿网的资深工程师徐磊，为大家分享关于 Elasticsearch 运维的那些事，跟小编一起走进现场吧~~
Elasticsearch 运维 Elasticsearch 在近两年越来越火了，越来越多的公司和团队尝试使用它支撑业务。运维人员如何保证 Elasticsearch 集群的稳定？有哪些必须掌握的优化技巧？
在本次分享中，徐磊老师从数据模型设计，使用技巧，参数优化，监控对比等多个方面为大家分析了 Elasticsearch 的优缺点和运维重点。同时与大家分享了内部的 Elasticsearch 私有云的建设经验。
 徐磊，2015 年加入去哪儿网平台事业部 OPSDEV 团队，负责实时日志系统的建设和运维工作，开源社区贡献者，曾供职于 Red Hat。
 干货节选 来~这里还有讲师的 PPT 节选，一起看看，在 Elasticsearch 中，有哪些要注意的坑吧~~</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.43】RocksDB 专场分享</title>
      <link>https://pingcap.com/meetup/meetup-43-20170325/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-43-20170325/</guid>
      <description>今天的 Meetup，是 RocksDB 专场哦~ :) 这一次，我们请到了来自 360 基础架构组的研发工程师，宋昭与赵安安，为大家分享了他们各自对 RocksDB 的独到见解及研发经验。没有机会来现场的小伙伴，这里有第一手现场资料~👇
Topic 1：RocksDB Write &amp;amp; Compaction Speaker：宋昭，360 基础架构组研发工程师
在本次分享中，宋昭老师基于 RocksDB 5.0.1 代码，为大家详细介绍了 RocksDB write、flush 及 compaction 的具体实现，并对照 LevelDB 来分析对比 RocksDB 在实现及策略上的各种优化。
以下是讲师 PPT 节选~~
Topic 2：RocksDB in Pika  Speaker：赵安安，360 基础架构组研发工程师
赵安安老师从 RocksDB 在 Pika 中的应用情况来为大家进行了以下讲解： Pika 如何用 KV 实现多数据结构；基于这一设计，如何定制修改 RocksDB 实现 TTL 和 key 的秒删；特定接口的一些优化；以及使用 RocksDB 的一些经验。
讲师 PPT 节选， Again~
应众多身在异地的小伙伴们的强烈要求，本期 Meetup 临时开启了线上直播，上图有位帅气的兼职主播 :-D</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.42】Spark 与机器学习</title>
      <link>https://pingcap.com/meetup/meetup-42-20170318/</link>
      <pubDate>Sat, 18 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-42-20170318/</guid>
      <description>今天的专场 Meetup 邀请到了 Apache Spark Committer 梁堰波～梁老师结合自己的经验为大家分享了话题《Spark 与机器学习》，让我们来回顾一下现场实况吧！
Spark 与机器学习 Apache Spark 已经成为业界标准的大规模数据处理的标准平台。Spark 的 MLlib 给机器学习工程师和数据科学家提供了一些最常用的机器学习算法库以及一个构建机器学习 pipeline 的工具。
作为国内活跃的 Spark Committer 之一，
梁老师对 Spark 的理解非常深刻，
他首先为大家分享了 Spark 的基本架构，
又举例说明了 Spark 能够解决的主要问题。
在本次 Meetup 上，
梁老师还分享了 MLlib 的主要算法、
如何扩展 MLlib 算法等姿势~
最后，梁老师通过实际的应用案例，分享了如何使用 Spark 构建机器学习 pipeline。
今天小伙伴们热情高涨~
讲师：梁堰波，Apache Spark Committer，开源爱好者，北京航空航天大学计算机硕士，曾就职于 Yahoo!、美团网、法国电信，具有大数据、数据挖掘和机器学习领域的项目经验。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.41】大容量 redis 存储中间件（onestore）架构实践 &amp; TiKV RC2 性能大幅提升的秘密</title>
      <link>https://pingcap.com/meetup/meetup-41-20170311/</link>
      <pubDate>Sat, 11 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-41-20170311/</guid>
      <description>今天的 Meetup，陌陌基础平台部门负责人杨建军与 PingCAP 核心研发工程师张金鹏，为大家分享了《大容量 redis 存储中间件（onestore）架构实践》与《TiKV RC2 性能大幅提升的秘密》两个话题 ，跟小编一起走进现场吧~
Topic 1: 大容量 redis 存储中间件（onestore）架构实践 从上线至今 5 年来，
陌陌用户快速增长，服务接口总访问量飙升，
其中存储总访问量达到千万级 /s。
这就要求后端存储的性能也随之提升，
稳定、高效的存储组件是解决这些问题的关键。
今天的 Meetup，
杨建军就为我们分享了
陌陌自研大容量存储中间件 onestore 发展过程、
架构选型、设计以及在陌陌应用情况。
以下是热气腾腾的干货PPT节选~
Speaker：杨建军，陌陌基础平台部门负责人。先后参与过存储中间件、服务化框架、 统一配置中心、分布式调用跟踪系统等中间件架构设计以及研发工作。个人目前主要关注高并发系统架构、中间件研发、大数据平台、存储、团队管理等方向。
Topic 2: TiKV RC2 性能大幅提升的秘密 3 月 1 日，TiDB 正式发布了 RC2 版。该版本对 MySQL 兼容性、SQL 优化器、系统稳定性、性能做了大量的工作，对于 OLTP 场景，读写性能都有大幅度的提升。
在本次分享中，张金鹏为我们剖析了 TiKV 获得大幅性能提升的秘密，包括使用异步 Apply 大幅提升写性能、使用 prefix seek 提升读取性能、引入 memtable insert with hint 提升 Raft CF 的插入性能同时减少 CPU 的使用，以及针对单行只读事务的优化等等。
之前 Raft 相关的工作都是在 raftstore 线程进行的，包括处理所有的 Raft 消息，把 raft log 持久化到 RocksDB，apply raft log 并将状态机的状态持久化到 RocksDB。写入繁忙时 raftstore 会成为系统的瓶颈，同时 write RocksDB 可能会发生 stall，此时容易造成 Raft 的 leader 切换导致系统抖动。将 write RocksDB 的工作以及一些占用 CPU 的工作从 raftstore 线程分离开来，一方面能够充分利用系统资源提升系统的性能，另一方面使系统更加稳定。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.40】tcp 链接的建立与释放 &amp; 分布式数据库中统计信息的收集和使用</title>
      <link>https://pingcap.com/meetup/meetup-40-20170304/</link>
      <pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-40-20170304/</guid>
      <description>今天的 Meetup，360 基础架构团队工程师吴晓飞和 PingCAP 研发工程师韩飞与大家分享了《tcp 链接的建立与释放》和《分布式数据库中统计信息的收集和使用》，快来看看现场吧~
Topic 1：tcp 链接的建立与释放 Speaker：吴晓飞
吴晓飞，360 基础架构团队工程师，曾参与 360 自主存储项目 bada 的开发，也是 360 开源项目 pika、mongosync 等主要开发者之一。
吴晓飞基于 linux-2.6.32 版本，
介绍了 tcp 连接在内核中的组织结构、
连接建立与释放的大致流程。 泼（P）泼（P）特（T）节选→_→
Topic 2：分布式数据库中统计信息的收集和使用 Speaker：韩飞
号称 PingCAP SQL 小王子的韩飞童鞋~
统计信息一般有四种做法：
采样（Sampling）、直方图（Histogram）、小波（Wavelet）和略图（Sketch），
韩飞童鞋围绕这四种方式进行了逐一的分析。
泼（P）泼（P）特（T）节选→_→
在分布式数据库中，直接使用采样的方式估算 Join Cost 或者计算 Range-Sum 虽然比较精准，但是会造成比较大的 overhead，而且对内存敏感，难以维护。一般对于非有序列会使用随即采样+全量排序的方式来构建直方图。
直方图的选择一般从两个方面去考量：一个是分桶策略（Bucket Scheme）；一个是每个桶的估算策略（Estimate Scheme）。
最后，在介绍了小波变换（Wavelet Transformation）和略图之后，韩飞童鞋结合 TiDB 的实践讨论了各种统计信息设计和使用方式的优劣，以及离线收集和在线更新的高效算法。
相信大家在这次 Meetup 里有所收获！</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.39】百度 Galaxy 集群管理系统</title>
      <link>https://pingcap.com/meetup/meetup-39-20170225/</link>
      <pubDate>Sat, 25 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-39-20170225/</guid>
      <description>今天的 COISF 专场 Meetup
我们邀请到了百度工程师郝立飞老师
与大家分享百度 Galaxy 集群管理系统
郝立飞从 Galaxy 的背景引入，
为大家详细介绍了
Galaxy 的设计和使用、Galaxy 的功能，
同时，还分享了 Galaxy 的一些具体应用。
以下是节选的 PPT 截图，share 给大家~
听了郝立飞老师分享，大家是不是又涨姿势了呢^^
大家好认真呐，希望有所收获！</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.38】深入解读 Google Spanner</title>
      <link>https://pingcap.com/meetup/meetup-38-20170218/</link>
      <pubDate>Sat, 18 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-38-20170218/</guid>
      <description>视频 | Google Cloud Spanner 小科普   Google 宣布了其 Cloud Spanner 云端数据库服务的 Beta 版测试启动。对此，各方报道层出，大家被新闻刷屏的同时也提出了不少疑问。本期 Meetup 邀请了我司 CEO 刘奇为大家深入解读 Google Spanner，同时与大家讨论了 Spanner 存在的一些限制以及背后的原因。
 开始之前，活动报名的人数已达上限
没有成功报名的小伙伴也准时赶来
很是感动~
Meetup 现场，刘奇为大家多层次详细对比介绍了 Google Spanner 和 TiDB：
Google Spanner 的事务模型和 TiDB 事务模型的差异；事务的隔离级别以及不同选择背后的原因；为什么 Spanner 选择悲观锁，而 TiDB 选择了乐观锁；数据分片大小对实际使用场景的影响。
此外，在分析了 Spanner 的最佳实践，以及架构对最佳实践的直接影响之后，刘奇还分享了 Spanner 的 TrueTime API 在跨数据中心的优势，推测未来 Google 会在云上提供 TrueTime API。并解读了 TiDB 在没有原子钟的情况下做了哪些优化来降低延迟，以及后续对调度器的改进计划。
满满的干货内容，伙伴们听嗨了，讲师也讲嗨了~
让我们一起走进活动现场吧~
……(⊙﹏⊙)
30 分钟自由讨论环节
本期 Meetup 特别设置了半小时的讨论环节
各位伙伴都热情满满地讨论并分享了自己的观点
更提出了一些疑问~
本次 Meetup 就在大家意犹未尽的会后讨论中~</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.37】PD 调度的实现 &amp; 主流分布式文件系统对比介绍</title>
      <link>https://pingcap.com/meetup/meetup-37-20170114/</link>
      <pubDate>Sat, 14 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-37-20170114/</guid>
      <description>今天是 COISF 专场 Meetup， PingCAP 工程师黄华超和百度网页搜索部基础架构工程师杨策分别与大家分享了《PD 调度的实现》以及《主流分布式文件系统对比介绍》。
Topic 1：PD 调度的实现 Speaker：
黄华超，PingCAP 工程师，曾就职于微信、好赞科技，从事分布式存储相关工作，现负责 PingCAP PD 研发工作。
Content：
本次分享主要介绍 PD 是如何实现资源调度的。PD 主要涉及三种调度，分别是 Leader，Storage 和 Replica 调度。Leader 调度是为了让集群的所有节点的负载均衡，Storage 调度的作用是让集群的所有节点的磁盘使用率均衡，Replica 调度则是为了让所有的 Region 有足够的副本数，包括节点故障或者是节点下线的调度。最后还介绍了如何在不同的数据中心、机架以及机器上进行调度。
Topic 2：主流分布式文件系统对比介绍 Speaker：
杨策，百度网页搜索部基础架构工程师，现主要从事分布式文件系统 BFS 相关的研发工作。
Content：
分布式文件系统是很多分布式系统里的重要组件，除了可以进行文件存储之外，在此之上可以构造诸如分布式数据库，消息队列等诸多系统。而根据这些系统对元数据量，可扩展性，可用性的不同需求，分布式文件系统又衍生出众多不同的设计和实现。
本次分享的主要内容是对主流的分布式文件系统进行对比介绍，使大家对于不同业务特性、对于分布式文件系统选型的影响有了进一步了解，同时介绍了百度文件系统BFS在设计和实现上的不同之处 。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.36】Tera 的单机存储引擎 &amp; 机器学习之我见</title>
      <link>https://pingcap.com/meetup/meetup-36-20170107/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-36-20170107/</guid>
      <description>今天是 COISF 专场 Meetup，分享的主题为《Tera 的单机存储引擎的设计与实现》以及《机器学习之我见》。
Topic 1：Tera 的单机存储引擎的设计与实现 Speaker：蔡杰明，百度网页搜索部基础架构工程师，现主要工作从事 Tera 相关的研发工作。
Content：Tera 是一个高性能、可伸缩的结构化数据存储系统，被设计用来管理搜索引擎万亿量级的超链与网页信息。Tera 的出现，使得众多业务从基于 map-reduce 批量计算转变为基于 Tera 的实时计算，结果的时效性获得巨大提升；而依赖于 Tera 的高可扩展，实现了业务的计算能力能轻松扩展到数千机器。
本次分享的主要内容是 Tera 的单机存储引擎的设计与实现。通过介绍单机存储引擎的原理，让大家了解 Tera 是如何实现高效的负载均衡和高性能的随机读写。
Topic 2：机器学习之我见 Speaker：袁进辉，2003 年 7 月以年级第一名毕业于西安电子科技大学计算机学院，并被免试推荐入清华大学计算机系攻读博士学位，师从张钹院士，研究方向为计算机视觉及机器学习，2008 年 7 月获得工学博士学位，博士论文获得清华大学优秀博士学位论文奖，同年留校做师资博士后。2004 至 2007 年参与美国国家技术标准局组织的视频检索评测，获得多项第一。2010 年，与国家体育总局合作，负责研发斯诺克比赛“鹰眼”系统，目前该系统服务于各项国际大赛，并被国家队作为日常训练辅助系统。2011 年加入网易有道，任高级应用研究员。 2012 年作为早期成员加入 360 搜索团队，一年之后，产品上线成为国内市场份额第二的搜索引擎。 2013 年加入微软亚洲研究院，主要从事大规模机器学习平台的研发工作。 2014 年，首次将训练 LDA 主题模型的吉布斯采样算法的计算复杂度降到单个词为常数级，基于该算法的分布式实现仅需数十台服务器即可完成以往数千台服务器才能完成的任务。 2015 年至今，专注于搭建基于异构集群的深度学习平台。 2016 年 11 月开始创业。工作之余，乐于在新浪微博讨论技术问题，绰号老师木。
Content：机器学习原理及应用。介绍机器学习的基本原理，它在数学和哲学上假设和依据，机器学习能做什么和不能做什么，应用机器学习解决实际问题时的几个关键点。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.35】Sofa-pbrpc 设计与实现 &amp; 自底向上纵览 TiKV 架构</title>
      <link>https://pingcap.com/meetup/meetup-35-20161224/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-35-20161224/</guid>
      <description>今天是平安夜，在圣诞钟声敲响之前，COISF 的专场 Meetup 也带着满满的平安夜祝福与大家相聚在周末，本周分享的主题是《Sofa-pbrpc 设计与实现》以及《自底向上纵览 TiKV 架构》。
Topic 1：Sofa-pbrpc 设计与实现 Speaker：
张頔，百度网页搜索部基础架构工程师。
Content：
Sofa-pbrpc 是一个基于 protobuf 的轻量级网络通信框架，在百度搜索架构中广泛应用。单机百万 QPS，接近 ping 的延迟，支持网络流控和多种开发语言。本次分享整体介绍了 Sofa-pbrpc 的设计特点和使用方法。
Topic 2：自底向上纵览 TiKV 架构 Speaker：
黄梦龙( disksing ) ，COISF TiKV Committer，开源爱好者，PingCAP 工程师，现主要负责 TiDB/TiKV 中分布式事务相关研发工作。
Content：
分布式开源 Key-Value 存储引擎 TiKV 是 TiDB 项目的重要组成部分，从项目立项至今不到一年时间，已经成功应用于多个产品的生产环境中，这得益于来自开源社区的大量帮助，同时也离不开项目自身良好的分层架构。
本次分享从自底向上的角度剖析分布式 TiKV 的各个组件，阐释其实现高可用、强一致、在线水平扩展、分布式事务的原理，分享在项目构建过程中的一些设计和思考。
分享提纲：</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.34】Pegasus：一个分布式 KV 系统的设计过程 &amp; TiDB 架构介绍</title>
      <link>https://pingcap.com/meetup/meetup-34-20161217/</link>
      <pubDate>Sat, 17 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-34-20161217/</guid>
      <description>今天是 COISF 专场 Meetup，为了感谢顶着帝都雾霾来听分享的真爱粉们，现场有为大家准备银耳雪梨甜汤哦 😊本周分享的主题是小米工程师覃左言带来的《Pegasus：一个分布式 KV 系统的设计过程》以及 PingCAP TiDB Tech Lead 申砾带来的《TiDB 架构介绍》。
Topic 1：《Pegasus：一个分布式 KV 系统的设计过程》 Speaker：
覃左言，COISF Pegasus PMC，小米工程师，目前主要在小米云平台负责分布式存储系统 Pegasus 的相关工作。专注基础框架和分布式系统，曾在腾讯搜搜基础架构部参与研发分布式存储系统 xcube，后来在百度网页搜索部设计与开发了微服务开发框架 SOFA。热衷开源，是开源 RPC 框架 sofa-pbrpc 的作者，也是分布式系统开发框架 rDSN 的重要贡献者。
Content：
随着小米公司业务量的快速增长，小米云平台迎来了越来越多的挑战。原有一些系统在服务业务的过程中，已经逐渐显现出了设计上的瓶颈和不足。譬如小米目前大量使用的 HBase 系统在实际应用中就遇到了一些痛点问题，包括 Java GC 假死造成的无响应、ZooKeeper 的 session 超时不够 敏锐、系统 Failover 过程较慢等。虽然有些问题可以通过优化来缓解，但还有些问题受限于架构本身难以得到根本解决。
为此，小米基于 C++ 开发了一套新的分布式 key-value 系统 Pegasus，以弥补 HBase 的不足，为在线和离线业务提供高可用、高性能、强一致、易使用的存储服务。在本次分享中，首先阐述了 Pegasus 系统产生的背景，重点介绍 Pegasus 系统的整个设计过程，并分享了在分布式系统开发中的一些经验。希望通过带领大家重走 Pegasus 的设计之路，让大家了解如何设计一个分布式存储系统，会遇到哪些问题，有哪些可能的解决思路。
Topic 2：《TiDB 架构介绍》 Speaker：
申砾，COISF TiDB PMC，PingCAP TiDB Tech Lead，前网易有道词典服务器端核心开发，前奇虎 360 新闻推荐系统 / 地图基础数据与检索系统 Tech Lead。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.33】zeppelin 的设计与实现 &amp; 海量结构化数据库 Tera 总览</title>
      <link>https://pingcap.com/meetup/meetup-33-20161210/</link>
      <pubDate>Sat, 10 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-33-20161210/</guid>
      <description>今天是 COISF 专场 Meetup，主题是 360 基础架构组高级研发工程师王康分享的《zeppelin 的设计与实现》以及百度网页搜索部工程师李康分享的《海量结构化数据库 Tera 总览》。
Topic 1：zeppelin 的设计与实现 Speaker：
王康，360 基础架构组高级研发工程师, 负责实现了 360 开源配置管理服务 QConf, 大容量 redis pika，zeppelin 主要设计和实现。
Content：
本次分享介绍了 360 半离线存储服务的设计与实现, 介绍 zeppelin 与 pika, bada 等在线存储的区别以及设计上的一些折衷。
Topic 2：海量结构化数据库 Tera 总览 Speaker：
李康，百度网页搜索部工程师，负责百度海量结构化数据库 Tera 的设计和开发工作。
Content：
Tera 是百度搜索基础架构团队为管理搜索引擎万亿量级的超链和网页数据设计的结构化数据库，当前已部署上万台机器，支持数十个生产应用。本次分享介绍了 Tera 产生背景和整体设计思路。</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿 - 信心的毁灭与重建</title>
      <link>https://pingcap.com/meetup/recording/distributed-system-test-3/</link>
      <pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/distributed-system-test-3/</guid>
      <description>本话题系列文章整理自 PingCAP Infra Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为下篇。
 -接中篇- ScyllaDB 有一个开源的东西，是专门用来给文件系统做 Failure Injection 的, 名字叫做 CharybdeFS。如果你想测试你的系统，就是文件系统在哪不断出问题，比如说写磁盘失败了，驱动程序分配内存失败了，文件已经存在等等，它都可以测模拟出来。
CharybdeFS: A new fault-injecting file system for software testing
Simulate the following errors:
 disk IO error (EIO) driver out of memory error (ENOMEM) file already exists (EEXIST) disk quota exceeded (EDQUOT)  再来看看 Cloudera，下图是整个 Cloudera 的一个 Failure Injection 的结构。
一边是 Tools，一边是它的整个的 Level 划分。比如说整个 Cluster， Cluster 上面有很多 Host，Host 上面又跑了各种 Service，整个系统主要用于测试 HDFS， HDFS 也是很努力的在做有效的测试。然后每个机器上部署一个 AgenTEST，就用来注射那些可能出现的错误。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.32】百度文件系统－面向实时应用的分布式文件系统 &amp; TiDB - The Future of Database</title>
      <link>https://pingcap.com/meetup/meetup-32-20161203/</link>
      <pubDate>Sat, 03 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-32-20161203/</guid>
      <description>今天是一期人数爆满的 Meetup。作为 COISF 专场，感谢众多小伙伴与我们一起见证 COISF 的首次亮相。当然，首场参与是一定会有福利滴。这一期，我们邀请到了一位女神级讲师&amp;ndash;百度网页搜索部工程师雷丽媛，为大家讲解百度文件系统的架构设计；另外，PingCAP 联合创始人崔秋也有出台，为大家深情回顾 TiDB 的发展历程 :)
开场：COISF Opening Talk 在本环节中，PingCAP Co-Founder 崔秋，百度搜索基础架构团队技术负责人颜世光，以及奇虎 360 基础架构组存储负责人陈宗志共同为大家介绍了 COISF 的由来和使命，并对目前基金会内的顶级项目进行了简单介绍。
COISF（China Open Infrastructure Software Foundation ：中国开放基础软件基金会，其核心技术委员会由 PingCAP、百度、奇虎 360、小米（排名不分先后）等公司的基础软件项目团队组成，致力于促进和发展中国的新一代开源基础软件。目前基金会项目包括：Baidu/BFS、Baidu/Tera、PingCAP/TiDB、PingCAP/TiKV、Qihoo360/Zeppelin 等。
我们认为，一方面开源是软件开发的未来，能更好地促进创新与合作；另一方面未来几十年中国的基础软件必将蓬勃发展，并在世界范围内扮演重要角色。但当前国内有很多优秀的开源软件, 因为文化和语言的藩篱没能融入西方社区, 无法获得足够的关注与支持，导致发展缓慢。我们通过建设中国统一的基础软件开发社区，甄选优秀的项目加入，集中优势资源促进这些项目的快速发展与成熟。
COISF 的使命是：促进中国下一代开源基础软件生态系统的发展。
Topic 1：百度文件系统－面向实时应用的分布式文件系统 Speaker：
雷丽媛，COISF BFS PMC，百度网页搜索部工程师，专注于分布式存储领域，目前负责百度结构化数据存储和分布式系统的相关工作。
Content：
百度的核心业务和数据库系统都依赖分布式文件系统作为底层存储，文件系统的可用性和性能对上层搜索业务的稳定性与效果有着至关重要的影响。现有的分布式文件系统（如HDFS等）是为离线批处理设计的，无法在保证高吞吐的情况下做到低延迟和持续可用，所以百度从搜索的业务特点出发，设计了百度文件系统。本场分享整体介绍了百度文件系统 BFS 的架构设计和子模块。
Topic 2：TiDB - The Future of Database Speaker：
崔秋，COISF TiDB PMC，PingCAP 联合创始人，重度开源爱好者，曾任职于搜狗、豌豆荚，长期从事广告系统基础组件相关的研究，现主要从事开源 NewSQL 数据库 TiDB/TiKV 相关的设计和研发工作。
Content：
NewSQL 不仅具有传统 SQL 和 ACID 的事务保证，同时还具有 NoSQL 的 Scale 能力, 这是一种世界前沿的数据库新技术。TiDB 以 Google Spanner/F1 作为理论参考，从零到一地完整实现这种面向未来的数据库。今天我们主要回顾 TiDB 的整个发展历程，从单机到分布式，从 Alpha 到 RC，从开源到社区，分享每一次架构演进背后的思考和感悟，以及每个不同阶段我们所做的取舍。最后，从客户的真实反馈中，我们一起探讨了 TiDB 的适用场景和最佳实践。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.31】PD 的实现和演进 &amp; 从容器和微服务的发展看基础架构变迁</title>
      <link>https://pingcap.com/meetup/meetup-31-20161126/</link>
      <pubDate>Sat, 26 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-31-20161126/</guid>
      <description>今天是 PingCAP 第 31 期 Meetup，主题是黄华超分享的《PD 的实现和演进》以及邓栓分享的《从容器和微服务的发展看基础架构变迁》。
Topic 1：PD 的实现和演进 Lecturer：
黄华超，PingCAP 工程师，曾就职于微信、好赞科技，从事分布式存储相关工作，现负责 PingCAP PD 研发工作。
Content：
本次分享首先介绍了 PD 在 TiDB 集群的作用，以及集群是如何动态扩容缩容的。然后分别讲解了 PD 的各个功能是如何实现的，其中，着重分享了集群调度的相关设计和思考，以及新的标签调度功能。
Topic 2：从容器和微服务的发展看基础架构变迁 Lecturer：
邓栓（Tennix），Rust 中文社区管理员，PingCAP SRE 工程师，负责 TiDB 与 Kubernetes 一体化整合部署方案。
Content：
近些年来容器和微服务的概念变得特别火热，越来越多的互联网公司开始尝试将以前的单体服务迁移到微服务，并且在实践中使用容器来部署服务，容器和微服务也催生了 DevOps，CaaS，Immutable infrastructure，Service orchestration 等概念。今天主要从容器和微服务角度谈了新技术应用和实践给开发者带来了哪些便利和挑战，基础架构发生了哪些改变，并尝试探讨了未来的应用服务会是什么样的架构。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.30】深度学习，众包数据与短时临近预报系统 &amp; 谈谈 TiDB-Binlog 的设计</title>
      <link>https://pingcap.com/meetup/meetup-30-20161119/</link>
      <pubDate>Sat, 19 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-30-20161119/</guid>
      <description>今天是 PingCAP 第 30 期 Meetup，主题是墨迹天气气象算法负责人刘锦龙分享的《深度学习，众包数据与短时临近预报系统》以及刘寅分享的《谈谈 TiDB-Binlog 的设计》。
Topic 1：深度学习，众包数据与短时临近预报系统 Lecturer：
刘锦龙，北大理论物理博士，墨迹天气气象算法负责人，负责墨迹相关天气预测算法的研发工作，主要方向为机器学习和深度学习。
Content：
深入介绍如何将深度学习的最新技术用于革新传统气象预测的一些研究和应用，以及如何处理从用户获取的众包反馈数据并进而改进天气预报的精准度。
Topic 2：谈谈 TiDB-Binlog 的设计 Lecturer：
刘寅，PingCAP engineer，现负责 TiDB 商业产品开发和自动化运维。
Content：
随着 TiDB 的不断稳定和完善，我们也逐步开发了很多 TiDB 周边工具。今天主要介绍了 TiDB-Binlog 设计上的一些考量和实现细节。
TiDB-Binlog 可实时记录 TiDB 的一切数据变化，可以用来做集群的实时备份和恢复，也可以将数据完整地实时同步到下游的异构数据平台。目前我们已经把 TiDB-Binlog 部署到真实客户的线上系统中，利用实时同步的特性保障了上线过程的可靠和数据安全。今天的分享着重介绍了 Binlog 的原理，以及生成、收集和还原的过程细节。</description>
    </item>
    
    <item>
      <title>MPP and SMP in TiDB</title>
      <link>https://pingcap.com/meetup/recording/mpp-smp-tidb/</link>
      <pubDate>Tue, 15 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/mpp-smp-tidb/</guid>
      <description>今天主要是想把我们 TiDB 做 SQL 性能优化的一些经验和一些思考，就此跟大家探讨一下。题目写的比较大，但是内容还是比较简单。我们做 TiDB 的 SQL 层时，一开始做的很简单，就是通过最简单的 KV 接口(Get/Set/Seek)去存数据、取数据，做一些非常直白、简单的计算。然而后来我们发现，这个方案在性能上不可接受，可能行不通，我们就重新思考了这个事情。
TiDB 的目标是做一个 NewSQL 的 database ，什么是 NewSQL？从 Wikipedia 上我们看到 NewSQL 的定义『NewSQL is a class of modern relational database management systems that seek to provide the same scalable performance of NoSQL systems for online transaction processing (OLTP) read-write workloads while still maintaining the ACID guarantees of a traditional database system.』。首先NewSQL Database 需要能存储海量数据，这点就像一些 NoSQL 数据库一样。然后，能够提供事务的功能。所以 NewSQL 中的计算，主要有两个特点。第一个，就是数据是海量的，这跟 MySQL 传统数据有可能不一样，他们当然可以通过一些 sharding 的方式来进行处理，但是 sharding 之后会损失，比如说你不能跨节点做 Join，没有跨节点事务等。二是，在海量数据情况下，我们还需要对数据进行随时的取用，因为数据存在那，你算不出来就是对用户没有价值、没有意义的，所以我们需要在海量数据的前提下，能够随时把它计算出来。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.29】映客直播服务端架构优化之路 &amp; MySQL 与 TiDB 的事务机制</title>
      <link>https://pingcap.com/meetup/meetup-29-20161112/</link>
      <pubDate>Sat, 12 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-29-20161112/</guid>
      <description>今天是 PingCAP 第 29 期 Meetup，主题是映客服务端架构师王振涛分享的《映客直播服务端架构优化之路》以及张金鹏分享的《MySQL 与 TiDB 的事务机制》。
Topic 1：映客直播服务端架构优化之路 Lecture：
王振涛，南开大学计算机硕士毕业，曾先后供职于腾讯、搜狗等互联网公司，拥有多年的服务端研发、面向服务体系结构设计经验，专注于解决海量数据存储和计算带来的分布式、高并发、强一致性等技术难题和挑战。2016 年初加入映客直播，担任服务端架构师，主要负责映客基础平台架构设计、评审和用户体系的研发工作，经历了映客业务快速发展、构建高可用大容量基础服务体系的过程，对分布式计算、微服务、分布式数据库架构、高可用高并发系统设计等方面都有较深刻的理解和实践经验。
Content：
1、介绍了映客服务端架构演进历程；
2、关于服务端技术选型的探索和思考；
3、移动直播典型应用场景分析。
Topic 2：MySQL 与 TiDB 的事务机制 Lecture：
张金鹏，PingCAP 核心成员，前百度资深研发工程师／京东数据库专家，《MariaDB 原理和实现》作者。
Content：
在 MySQL 的 InnoDB 存储引擎中，进行写操作时，会将数据修改前的状态纪录在 Undo Log 中，一旦事务，失败利用 Undo Log 来进行回滚，保证事务的原子性。同时 InnoDB 利用 Undo Log 实现了多版本并发控制，InnoDB 的读取操作是不加锁的，事务只能读取到事务开始时已提交的纪录。由于 MySQL 是单机数据库，所有很方便的纪录所有活跃的事务 ID，Purge 线程根据当前活跃的事务情况来定期清理 Undo Log 中过期版本的数据。InnoDB 的事务支持 read uncommitted、read committed、repeatable read、serializable 四种事务隔离级别，InnoDB 通过 next-key lock 来解决 repeatable read 隔离级别下的幻读现象。
由于 TiDB 是分布式的数据库，情况变的复杂一些。TiDB 的事务参考的是 Google 的 percolator 模型，通过 PD 获取单调递增的时间戳来作为事务编号。TiDB 的写分为 prewrite 和 commit 两个阶段。如果一个事务写入多行，会选取一行作为 primary row，当 prewrite 阶段成功后会 commit primary row，其他 row 根据 primary row 的提交结果选择提交或者回滚，以保证整个事务的原子性。TiDB 同时实现了 SI 和 SSI 两种事务隔离级别。</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿 - 错误注入</title>
      <link>https://pingcap.com/meetup/recording/distributed-system-test-2/</link>
      <pubDate>Thu, 10 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/distributed-system-test-2/</guid>
      <description>本话题系列文章整理自 PingCAP Infra Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为中篇。
 -接上篇- 当然测试可能会让你代码变得没有那么漂亮，举个例子：
这是知名的 Kubernetes 的代码，就是说它有一个 DaemonSetcontroller，这 controller 里面注入了三个测试点，比如这个地方注入了一个 handler ，你可以认为所有的注入都是 interface。比如说你写一个简单的 1+1=2 的程序，假设我们写一个计算器，这个计算器的功能就是求和，那这就很难注入错误。所以你必须要在你正确的代码里面去注入测试逻辑。再比如别人 call 你的这个 add 的 function，然后你是不是有一个 error？这个 error 的问题是它可能永远不会返回一个 error，所以你必须要人肉的注进去，然后看应用程序是不是正确的行为。说完了加法，再说我们做一个除法。除法大家知道可能有处理异常，那上面是不是能正常处理呢？上面没有，上面写着一个比如说 6 ÷ 3，然后写了一个 test，coverage 100%，但是一个除零异常，系统就崩掉了，所以这时候就需要去注入错误。大名鼎鼎的 Kubernetes 为了测试各种异常逻辑也采用类似的方式，这个结构体不算长，大概是十几个成员，然后里面就注入了三个点，可以在里面注入错误。
那么在设计 TiDB 的时候，我们当时是怎么考虑 test 这个事情的？首先一个百万级的 test 不可能由人肉来写，也就是说你如果重新定义一个自己的所谓的 SQL 语法，或者一个 query language，那这个时候你需要构建百万级的 test，即使全公司去写，写个两年都不够，所以这个事情显然是不靠谱的。但是除非说我的 query language 特别简单，比如像 MongoDB 早期的那种，那我一个“大于多少”的这种，或者 equal 这种条件查询特别简单的，那你确实是不需要构建这种百万级的 test。但是如果做一个 SQL 的 database 的话，那是需要构建这种非常非常复杂的 test 的。这时候这个 test 又不能全公司的人写个两年，对吧？所以有什么好办法呢？MySQL 兼容的各种系统都是可以用来 test 的，所以我们当时兼容 MySQL 协议，那意味着我们能够取得大量的 MySQL test。不知道有没有人统计过 MySQL 有多少个 test，产品级的 test 很吓人的，千万级。然后还有很多 ORM， 支持 MySQL 的各种应用都有自己的测试。大家知道，每个语言都会 build 自己的 ORM，然后甚至是一个语言的 ORM 都有好几个。比如说对于 MySQL 可能有排第一的、排第二的，那我们可以把这些全拿过来用来测试我们的系统。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.28】Spark 架构设计要点剖析 &amp; Performing group-by before join</title>
      <link>https://pingcap.com/meetup/meetup-28-20161105/</link>
      <pubDate>Sat, 05 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-28-20161105/</guid>
      <description>今天是 PingCAP 第 28 期 Meetup，主题是 TalkingData 数据经理时延军分享的《Spark 架构设计要点剖析》以及韩飞分享的《Performing group-by before join》。
Topic 1：Spark 架构设计要点剖析 Lecture：
时延军，TalkingData 数据经理，负责领域工程数据平台架构和研发，曾在 COMODO 中国负责基础数据平台建设，在车语传媒考拉 FM 负责后端数据平台架构（支持离线+实时分析处理）。推崇工程师文化，热爱开源，乐于分享，兴趣广泛，熟悉大数据技术生态，擅长软件系统架构、分布式计算系统设计。
Content：
1、RDD 特性，RDD 是如何抽象数据集的；
2、详解 Spark 基本架构；
3、Spark 内部核心组件及其交互；
4、逻辑执行计划与物理执行计划；
5、Spark 资源管理与任务调度。
Topic2：Performing group-by before join Lecture：
韩飞，PingCAP 研发工程师（PingCAP SQL 小王子），TiDB SQL Optimizer 主要作者，专注于 SQL 优化技术。前阿里云研发工程师，参与开发 ODPS SQL 查询优化器 Lot。
Content：
Efficient processing of aggregation queries is essential for decision support applications. This talk introduces a class of query trans-formations, called eager aggregation that allows a query optimizer to push group-by operations down the query tree.</description>
    </item>
    
    <item>
      <title>分布式系统测试那些事儿 - 理念</title>
      <link>https://pingcap.com/meetup/recording/distributed-system-test-1/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/distributed-system-test-1/</guid>
      <description>本话题系列文章整理自 PingCAP NewSQL Meetup 第 26 期刘奇分享的《深度探索分布式系统测试》议题现场实录。文章较长，为方便大家阅读，会分为上中下三篇，本文为上篇。
 今天主要是介绍分布式系统测试。对于 PingCAP 目前的现状来说，我们是觉得做好分布式系统测试比做一个分布式系统更难。就是你把它写出来不是最难的，把它测好才是最难的。大家肯定会觉得有这么夸张吗？那我们先从一个最简单的、每个人都会写的 Hello world 开始。
A simple “Hello world” is a miracle We should walk through all of the bugs in:
 Compiler Linker VM (maybe) OS  其实这个 Hello world 能够每次都正确运行已经是一个奇迹了，为什么呢？首先，编译器得没 bug，链接器得没 bug ；然后我们可能跑在 VM 上，那 VM 还得没 bug；并且 Hello world 那还有一个 syscall，那我们还得保证操作系统没有 bug；到这还不算吧，我们还得要硬件没有 bug。所以一个最简单程序它能正常运行起来，我们要穿越巨长的一条路径，然后这个路径里面所有的东西都不能出问题，我们才能看到一个最简单的 Hello world。
但是分布式系统里面呢，就更加复杂了。比如大家现在用的很典型的微服务。假设你提供了一个微服务，然后在微服务提供的功能就是输出一个 Hello world ，然后让别人来 Call。
A RPC “Hello world” is a miracle We should walk through all of the bugs in:</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.27】Impala 在用户行为分析中的应用与优化 &amp; How we build CI/CD for TiDB at scale</title>
      <link>https://pingcap.com/meetup/meetup-27-20161029/</link>
      <pubDate>Sat, 29 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-27-20161029/</guid>
      <description>今天是 PingCAP 第 27 期 Meetup，主题是神策数据联合创始人&amp;amp;首席架构师付力力分享的《Impala 在用户行为分析中的应用与优化》以及刘寅分享的《How we build CI/CD for TiDB at scale》。
Topic 1：Impala 在用户行为分析中的应用与优化 多冷的天都不能阻止技术童鞋们浓厚的求知欲 :-D
Lecture：
付力力，神策数据联合创始人&amp;amp;首席架构师，曾任百度、豌豆荚资深研发工程师，熟悉大规模数据处理、数据仓库、OLAP 数据库等领域。
Content：
1. 介绍用户行为分析的典型应用场景；
2. 简单介绍 Impala 的架构和实现；
3. 使用 Impala 进行用户行为分析的基本做法；
4. 针对特定场景对 Impala 进行的一些优化和改造。
Topic 2：How we build CI/CD for TiDB at scale Lecture：
刘寅，PingCAP engineer，现负责 TiDB 商业产品开发和自动化运维。
Content：
主要分享了我们如何为分布式数据库 TiDB 构建持续集成和持续交付平台，以支撑 TiDB 背后上千万的自动化测试 case，和多平台构建及发布。
其中，重点介绍了以 Jenkins 为核心的开源工具，配合 Docker ／ Kubernetes 来搭建分布式可扩展的 CI/CD 系统。Jenkins 2.0 之后的 pipeline script 的支持极大地提升分布式构建的灵活性，我们可以明确定义整个构建过程的不同阶段，并且决定这些阶段运行在集群的某个节点上，让耗时的任务并行处理，极大缩短从代码提交到上线发布的周期。同时结合实际场景的例子，讲解了 jenkins 的一些实用技巧和我们遇到的坑。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.26】京东分布式数据库实践 &amp; 深度探索分布式系统测试</title>
      <link>https://pingcap.com/meetup/meetup-26-20161022/</link>
      <pubDate>Sat, 22 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-26-20161022/</guid>
      <description>今天是 PingCAP 第 26 期 Meetup，主题是开源项目 speedy 作者张成远分享的《京东分布式数据库实践》以及刘奇分享的《深度探索分布式系统测试》。我司 CEO 亲自出台，现场不时传来三观碎一地的声音
另外，本周初次试水直播 (✿◡‿◡)
Topic 1：京东分布式数据库实践 Lecture：
张成远，《Mariadb 原理与实现》作者，开源项目 speedy 作者。目前就职于京东数据库系统研发团队，负责京东分布式数据库系统架构与研发工作，主导了京东分布式数据库系统在公司的落地及大规模推广。擅长高性能服务器开发，擅长分布式数据库/存储/缓存等大规模分布式系统架构。
Content：
 介绍京东分布式数据库的设计与实现；
 介绍去 oracle 的发展历程以及遇到的一些坑；
 如何做到高效的运维监控等。
  Topic 2：深度探索分布式系统测试 现场已爆满，本张照片拍摄于门缝&amp;hellip;
Lecture：
刘奇，PingCAP 联合创始人兼 CEO，先后创建了 Codis、TiDB/TiKV 等知名开源项目。现从事开源的分布式 NewSQL 数据库 TiDB/TiKV 开发。擅长高并发、大规模、分布式数据库系统架构设计。
Content：
主讲人自我点评称：“这是一次毁三观的分享”，因为这里定义了什么是及格的测试。如果您曾经认为自己的分布式系统测试做得非常好，听完之后，您会发现自己可能还远不到好的级别。
分布式系统测试是很少被提及的话题，但分布式系统测试的困难甚至大于写一个分布式系统。一般大家普遍的看法是平时用得很多的分布式系统都是比较稳定的，然而当新的测试方法和工具出现时，可以发现很多新的 bug 或者极大的提高了测试的复现率。复现率是解决 bug 的基础，分布式系统 bug 的复现难度也远大于单机系统。
本周刘奇和大家分享了分布式系统测试的一些困难，以及 PingCAP 和其它大型分布式系统的测试经验。为了方便未到现场的童鞋，后续小编会将本次分享内容整理成文档共享出来，让我们一起，毁~三~观~ :)</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.25】分布式数据处理在个性化系统的应用 &amp; TiKV 性能优化</title>
      <link>https://pingcap.com/meetup/meetup-25-20161015/</link>
      <pubDate>Sat, 15 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-25-20161015/</guid>
      <description>今天是 PingCAP 第 25 期 Meetup，顶着帝都的大雾霾，依然来了很多小伙伴。这一次我们有换新场地噢，但不变的是分享内容依然满满干货。本周的主题分别是百分点集团高级架构师武毅分享的《分布式数据处理在个性化系统的应用》以及张金鹏分享的《TiKV 性能优化》。
Topic 1：分布式数据处理在个性化系统的应用 Lecture：
武毅，现任百分点集团高级架构师，负责大数据平台基础架构的设计与研发，曾参与个性化推荐系统等多个大型系统的设计和开发。Linux 爱好者，活跃于 GitHub，Ubuntu 等社区，重点关注分布式技术，平台技术。
Content：
相信大家也都在各自的领域用到过不同的分布式存储／计算开源工具，本周我们分享了一些在运营个性化系统时使用分布式存储／计算工具遇到的坑和经验。
Topic 2：TiKV 性能优化 Content：
RocksDB 的 Column Families 之间会共享 WAL，但是又有各自的 memtables 和 sst files，共享 WAL 使得实现跨 CF 的 atomic 操作变成可能，不同 CF 的 memtables 和 sst files 是分离开的，这样我们可以将不同类型的数据分别存放在不同的 CF 内，根据数据的性质给 CF 定制不同配置，使数据的写入和访问达到最佳状态。
在目前 TiKV 中，读命令只能发给 leader，以防读取到旧的状态，在之前的版本中通过走一次 Raft 来确定当前节点是否是 leader，引入 leader lease 之后，命令发送到在 lease 内的leader 上时，不需要再走一次 Raft 了，可以直接读取本地数据。
当 RocksDB tombstone keys 太多的时候 seek 操作会非常慢，可以根据情况使用 iterator 的 upper bound 功能或者使用 RocksDB 的 singledelete 来解决这个问题.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.24】LLVM 简介及其在大规模 OLAP 中的应用 &amp; 阻塞访问数据库的相关问题</title>
      <link>https://pingcap.com/meetup/meetup-24-20160924/</link>
      <pubDate>Sat, 24 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-24-20160924/</guid>
      <description>今天是 PingCAP 第 24 期 Meetup，主题是阿里云 ODPS 研发工程师杜川分享的《LLVM 简介及其在大规模 OLAP 中的应用》以及来自小米云平台的杨哲分享的《阻塞访问数据库的相关问题》。
Topic 1：LLVM 简介及其在大规模 OLAP 中的应用 Lecture：
杜川，阿里云 ODPS 研发工程师，分布式数据库爱好者，重点关注 SQL 运行时优化以及 Code Generation 技术。
Content：
LLVM 是一个开源的编译器框架及生态链，已在工业界得到广泛的应用（著名的 Clang 编译器就是基于LLVM实现的）。因其前后端分离，模块化等优势，近年来被引入数据库领域，作为 JIT Code Generation 的工具，并吸引了越来越多的关注。本次分享介绍了 LLVM，及其在大规模 OLAP 中的应用。
Topic 2：阻塞访问数据库的相关问题 Lecture：
杨哲，id 杨肉或 yangzhe1991，现就职于小米云平台存储组。曾就职于网易有道、豌豆荚任资深工程师等职位。主要研究分布式数据库，在小米、有道、豌豆荚分别负责 HBase、Cassandra 和 Codis 的开发与维护。
Content：
分享了关于数据库若干问题的一些想法。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.23】How to write a good commit message &amp; QuorumKV：微信分布式 KV 存储系统</title>
      <link>https://pingcap.com/meetup/meetup-23-20160910/</link>
      <pubDate>Sat, 10 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-23-20160910/</guid>
      <description>今天是 PingCAP 第 23 期 Meetup，主题是金坤分享的《How to write a good commit message》以及黄华超分享的《QuorumKV：微信分布式 KV 存储系统》。
【Topic 1】How to write a good commit message Content：
This talk about writing good commit messages aims to act as the beginning of a series of talks about writing quality technical content. To emphasise the importance of the commit messages, the talk asked the audience to set up a profile of the potential reviewer who is as cool and picky as the writer of the technical content, or the writer himself in 5 years.</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.22】360开发的大容量redis -pika &amp; 分布式对象存储系统设计介绍</title>
      <link>https://pingcap.com/meetup/meetup-22-20160903/</link>
      <pubDate>Sat, 03 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-22-20160903/</guid>
      <description>今天是 PingCAP 第 22 期 Meetup，主题是 360 基础架构组研发工程师宋昭分享的《360 开发的大容量 redis -pika》以及美团云工程师张帅分享的《分布式对象存储系统设计介绍》。
Topic 1：360 开发的大容量 redis -pika Lecture：
宋昭，360 基础架构组研发工程师。专注于分布式存储领域，目前负责 360 开源项目 pika 相关的设计和开发工作。
Content：
目前 pika 在 360 内部大量使用，有 300 多实例，主要解决大容量的 redis（400G,800G）场景；在外部，被微博、美团、万达电商、garena、apus 等使用于线上核心系统中。本次分享主要介绍 pika 的系统设计和实现。
Topic 2：分布式对象存储系统设计介绍 Lecture：
张帅，美团云工程师。对分布式数据库及分布式存储系统有浓厚的兴趣。
Content：
分享关于大规模分布式对象存储的一些想法和思考。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.21】An Introduction to Join-Reorder in TiDB &amp; MPP and SMP in TiDB</title>
      <link>https://pingcap.com/meetup/meetup-21-20160827/</link>
      <pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-21-20160827/</guid>
      <description>今天是 PingCAP 第 21 期 Meetup，主题是韩飞分享的《An Introduction to Join-Reorder in TiDB》以及申砾分享的《MPP and SMP in TiDB》。
Topic 1：An Introduction to Join-Reorder in TiDB Content：
本次分享详细介绍了 TiDB 中 Join-Reorder 的流程。包括 Join-Reorder 的动机，outer-join 的 reorder 局限性和解决办法。为了解决某些 outer join re-association 的问题，我们可以引入的新算子 Generalized outerJoin。最后介绍了通过为 Join Query 建立 Query Graph 进行启发式搜索和动态规划的 Join-Reorder 算法。
Topic 2：MPP and SMP in TiDB Content：
TiDB 是一个支持水平扩展的分布式数据库，除了提供海量数据存储能力之外，还需要提供海量数据的计算能力，这样才能帮助用户更好、更容易地使用数据。为此我们开发了一套分布式计算框架，一方面利用海量的存储节点的计算能力，加快数据处理速度；另一方面在单个计算节点内，我们利用 Go 的并发优势，通过 SMP 方式提高计算并行度。
本次 Talk 首先介绍了 TiDB 分布式计算架构，并举例说明计算的具体流程；然后分享了最近 TiDB 针对索引查询和 Join 做的一系列优化，性能有大幅度提高；最后列出了一些 NewSQL database 中如何做计算值得思考的问题。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.20】搜索引擎背后的万亿量级存储系统 Tera &amp; Cloudtable：分布式强一致的 KV 存储系统</title>
      <link>https://pingcap.com/meetup/meetup-20-20190820/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-20-20190820/</guid>
      <description>今天是 PingCAP 第 20 期 Meetup，主题是百度网页搜索部工程师雷丽媛分享的《搜索引擎背后的万亿量级存储系统 Tera 》以及温文鎏分享的《Cloudtable：分布式强一致的 KV 存储系统》。
Topic 1：搜索引擎背后的万亿量级存储系统 Tera  Lecture：
雷丽媛，百度网页搜索部工程师。专注于分布式存储领域，目前负责百度结构化数据存储和分布式文件系统的相关工作。
Content：
介绍支撑搜索引擎核心的海量存储——Tera 的设计与实现
Topic 2：Cloudtable：分布式强一致的 KV 存储系统 Content：
如何搭建一个适用于互联网公司业务的大容量分布式强一致性 KV 存储系统?
通过结合分布式一致性协议 Raft，嵌入式存储引擎 RocksDB，HBASE 的架构和接口，YY 云存储团队在过去的两年开发了 Cloudtable 存储系统，它是一个分布式强一致性的 KV 存储系统。今天，前 YY 云存储工程师温文鎏分享了他们在构建 Cloudtbable 系统的实践和经验。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.19】What&#39;s New in Spark 2.0 &amp; An Overview of Cost Based Optimization and Join Reorder</title>
      <link>https://pingcap.com/meetup/meetup-19-20160806/</link>
      <pubDate>Sat, 06 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-19-20160806/</guid>
      <description>今天是 PingCAP 第 19 期 Meetup，主题是百度基础架构部工程师方君分享的《What&amp;rsquo;s New in Spark 2.0 》以及韩飞分享的《An Overview of Cost Based Optimization and Join Reorder》。
Topic 1：What&amp;rsquo;s New in Spark 2.0 Lecture：
方君，百度基础架构部工程师，专注于分布式计算与流式计算领域，目前在百度负责 Spark 计算平台和计算表示层的相关工作。
Content:
 DataSet API Performance Optimization Structure Streaming  Topic 2：An Overview ofCost Based Optimization and Join Reorder Content:
自从 System R 优化框架面世，基于 interesting order 的动态规划算法一直是大部分优化器采用的基础算法。本次分享介绍了优化器在没有 histogram 信息下的代价估计算法，以及举例说明 TiDB 中的动态规划算法实现。
最近有好多小伙伴在微信后台留言，想加入到我们的 Meetup 中来。在这里统一答复大家：我们的 Meetup 是每周六上午十点，在 PingCAP 公司内开讲哦。有兴趣的小伙伴届时带着你们对技术满满的热情来参加就好啦 :)</description>
    </item>
    
    <item>
      <title>TiDB 中的子查询优化技术</title>
      <link>https://pingcap.com/meetup/recording/tidb-optimization-for-subquery/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/tidb-optimization-for-subquery/</guid>
      <description>子查询简介 子查询是嵌套在另一个查询中的 SQL 表达式，比较常见的是嵌套在 FROM 子句中，如 SELECT ID FROM (SELECT * FROM SRC) AS T。对于出现在 FROM 中的子表达式，一般的 SQL 优化器都会处理的很好。但是当子查询出现在 WHERE 子句或 SELECT 列表中时，优化的难度就会大大增加，因为这时子查询可以出现在表达式中的任何位置，如 CASE...WHEN... 子句等。
对于不在 FROM 子句出现的子查询，分为“关联子查询”(Correlated Subquery) 和“非关联子查询”。关联子查询是指子查询中存在外部引用的列，例如：
SELECT * FROM SRC WHERE EXISTS(SELECT * FROM TMP WHERE TMP.id = SRC.id) 对于非关联子查询，我们可以在 plan 阶段进行预处理，将其改写成一个常量。因此，本文只考虑关联子查询的优化。
一般来说，子查询语句分为三种：
 标量子查询（Scalar Subquery），如(SELECT&amp;hellip;) + (SELECT&amp;hellip;)
 集合比较（Quantified Comparision），如T.a = ANY(SELECT&amp;hellip;)
 存在性测试（Existential Test），如NOT EXISTS(SELECT&amp;hellip;)，T.a IN (SELECT&amp;hellip;)
  对于简单的存在性测试类的子查询，一般的做法是将其改写成 SEMI-JOIN。但是很少有文献给出通用性的算法，指出什么样的查询可以“去关联化”。对于不能去关联化的子查询，数据库的做法通常是使用类似 Nested Loop 的方式去执行，称为 correlated execution。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.18】Kudu 的设计思想和具体实现 &amp; Kubernetes in PingCAP</title>
      <link>https://pingcap.com/meetup/meetup-18-20160730/</link>
      <pubDate>Sat, 30 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-18-20160730/</guid>
      <description>今天是 PingCAP 第 18 期 Meetup，主题是小米云平台工程师常冰琳分享的《Kudu 的设计思想和具体实现》以及张阳分享的《Kubernetes in PingCAP》。
Topic 1：Kudu 的设计思想和具体实现 lecture：
常冰琳，小米云平台工程师，长期专注于 Hadoop 生态的分布式计算框架，Kudu PMC&amp;amp;Commiter, Hadoop Nativetask 项目发起者(已合入 Hadoop)。目前在小米负责 SQL 类数据分析平台，利用 Impala 和 Kudu 搭建实时数据分析云服务。
Content：
本次分享将简单介绍 Kudu 的设计思想和具体实现，以及小米作为 Kudu 最早用户的一些实践经验。
 设计目标
 数据模型，分区和副本设计
 Tablet 存储设计
 其他底层细节
 小米实践
  Topic 2：Kubernetes in PingCAP Content：
本次分享，主要与大家沟通了 Kubernetes 在 TiKV 及 TiDB 中的一些应用场景，包括部署、运维以及与 Jenkins CI 的集成等。同时，对大家集中提问的 stateful 的 TiKV 在 rolling update、recovery 等情况下的“状态”维护上的一些问题，进行了探讨，基于此问题，大家在分享结束后也积极交流了各自对于 Kubernetes 本身的一些见解。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.17】How does TiKV auto-balance work?</title>
      <link>https://pingcap.com/meetup/meetup-17-20160723/</link>
      <pubDate>Sat, 23 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-17-20160723/</guid>
      <description>今天是 PingCAP 第 17 期 Meetup，主题是崔秋分享的《How does TiKV auto-balance work?》。
Topic：How does TiKV auto-balance work? TiDB 最近发布了 Beta 版本，相比传统的关系型数据库，TiDB 具有在线弹性伸缩，高可用和强一致性，一致性的分布式事务和 MySQL 协议兼容性等特性，特别适用于大规模高并发的海量数据场景。
本次交流主要介绍了 TiKV 的 Balance Scheduler 框架和算法实现演进，对于大家主要关注的 TiKV 集群的在线弹性扩容实现细节和 TiKV Balance 中在线服务高可用的问题，进行了深度的探讨。
在 TiKV 里面，数据是按照 Range 进行存放的，称为一个 Region。PD(Placement Driver) 负责整个 TiKV 集群的管理和调度。
在 TiKV 里面，数据移动的基本单元是 Region，所以 PD 的 auto balance 也是针对 Region 进行处理。对于一个 Region 来说，它会不会被 Balance，有两种方式：
1）Heartbeart
Region 会定期地上报当前的状态信息给 PD，如果 PD 发现该 Region 副本数不足或者超过阀值，则会通知该 Region 进行 Membership Change 处理。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.16】基于 Ceph 构建文件共享服务的实践 &amp; Cool Extensions of Raft for NewSQL</title>
      <link>https://pingcap.com/meetup/meetup-16-20160716/</link>
      <pubDate>Sat, 16 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-16-20160716/</guid>
      <description>今天是 PingCAP 第 16 期 Meetup，主题是来自京东的田琪分享的《Cool Extensions of Raft for NewSQL》，以及来自百度的孟圣智分享的《基于 Ceph 构建文件共享服务的实践》 。
Topic 1：Cool Extensions of Raft for NewSQL lecturer：
田琪，京东数据库系统部负责人，开源 docker 镜像存储系统 speedy 作者，TiDB committer, etcd contributor
Topic summary:
主要分享了 Raft 协议在 etcd 中的实现，与 etcd 在 Raft 协议方面近期更新地比较重要的特性，以及引进这些特性的缘由。
 the functionality of leader transfer
 the future improvement of leader transfer
 the functionality of quorum checking
 implement leader lease based on quorum checking
 some issues about leader lease</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.15】TiDB 存储模型变更 &amp; TiDB 优化器统计信息的采集</title>
      <link>https://pingcap.com/meetup/meetup-15-20160709/</link>
      <pubDate>Sat, 09 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-15-20160709/</guid>
      <description>今天是 PingCAP 第 15 期 Meetup ，主题是申砾分享的《TiDB 存储模型变更》以及周昱行分享的《TiDB 优化器统计信息的采集》。
Part 1：《TiDB 存储模型变更》 TiDB 在 Key-Value 存储模型之上，将一行数据拆分成多个 Key-Value pair。这样做有利于列较多并且 update 较为频繁的业务场景，同时对 Online Schema 变更较为友好。但是这种存储模型对于需要读取/写入大量 row 的业务场景并不适用。为此我们修改了 TiDB 的存储模型，将一行内需要频繁修改和很少修改的数据存储在不同的 column family 中，以更好地适应不同热度的数据,以及生存期差别比较大的数据。同时，非常有效地适配了读写放大以及空间放大的问题。
Part 2：《TiDB 优化器统计信息的采集》 统计信息是实现基于代价的优化（CBO）的必要条件，本期为大家介绍 TiDB 收集统计信息使用的采样算法和直方图生成算法。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.14】HashData 数据仓库的动态缩容扩容实现 &amp; TiDB Beta 版现场演示</title>
      <link>https://pingcap.com/meetup/meetup-14-20160702/</link>
      <pubDate>Sat, 02 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-14-20160702/</guid>
      <description>今天是 PingCAP 第14期 Meetup ，主题是酷克数据联合创始人马涛分享的《HashData 数据仓库的动态缩容扩容实现》以及 PingCAP 联合创始人兼 CEO 刘奇针对近日发布的 TiDB Beta 版进行的现场 Demo 演示。
Part 1：《 HashData 数据仓库的动态缩容扩容实现》 讲师：马涛，酷克数据联合创始人，数据库领域从业近10年，最初 Pivotal HAWQ 项目成员，06年至11年就职人大金仓做内核开发。目前主要负责 OLAP 系统内核和外围云化工作。
通过对比 Greenplum，Dynamo 和 HashData 的当前实现，为大家简单介绍数据处理系统动态缩容扩容的实现。阐述数据系统缩容和扩容的需求集合和设计方案，深入介绍 HashData 选择的设计、目前实现和后续改进。
Part 2：《 TiDB Beta 版现场 Demo 演示》 讲师：刘奇，PingCAP 联合创始人兼 CEO。
针对6月30日发布的 TiDB Beta 版，刘奇在现场进行演示，与大家共同见证了 TiDB 界面的首次亮相。直接通过标准的 MySQL 客户端连接，后端三台普通 x86 服务器集群，演示了常用的 SQL 插入和查询，并演示了在大压力数据写入的场景下，TiDB 自动扩容的全过程，期间无需人为干预，TiDB 自动完成数据迁移和扩容及流量的负载均衡，业务层完全透明。小伙伴们都惊呆了。
TiDB Beta 版已如约亮相，说好的 “三五好友，吃吃喝喝”，说来就来 :)</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.13】百度 redis3 生产环境实践 &amp; TiKV Auto Balance</title>
      <link>https://pingcap.com/meetup/meetup-13-20160625/</link>
      <pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-13-20160625/</guid>
      <description>今天是 PingCAP 第 13 期 Meetup ，主题是百度资深研发工程师、百度 BAC 存储负责人闫宇分享的《百度 redis3 生产环境实践》以及 PingCAP 联合创始人崔秋分享的《TiKV Auto Balance 》。
Topic 1：《百度 redis3 生产环境实践》 讲师：闫宇，百度资深研发工程师，百度 BAC 存储负责人
（百度 BAC 的 redis3 服务目前机器规模达到1400台左右，总数据量接近100T，日 pv 超过1500亿，用户涵盖了百度贴吧、百度糯米、手机百度等百度内部几百个业务线。）
内容方向：
1）介绍百度 BAC 的 redis3 服务的整体架构；
2）交流在 redis3 实践中的一些经验。
以下为本次分享的干货PPT：
Topic 2：《TiKV Auto Balance》 讲师：崔秋，PingCAP 联合创始人
内容方向：
1）PD - God View of TiKV； 2）TiKV 如何成为真正意义上的分布式存储引擎。
以下为本次分享的干货PPT：
最后，附赠一张今日爆满全场听讲图</description>
    </item>
    
    <item>
      <title>TiDB 下推 API 实现细节 - Union Scan</title>
      <link>https://pingcap.com/meetup/recording/tidb-api-union-scan/</link>
      <pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/tidb-api-union-scan/</guid>
      <description>TiDB 集群的架构分为上层的 SQL 层和底层的 KV 层，SQL 层通过调用 KV 层的 API 读写数据，由于 SQL 层的节点和 KV 层节点通常不在一台机器上，所以，每次调用 KV 的 API 都是一次 RPC, 而往往一个普通的 Select 语句的执行，需要调用几十到几十万次 KV 的接口，这样的结果就是性能非常差，绝大部分时间都消耗在 RPC 上。
为了解决这个问题，TiDB 实现了下推 API，把一部分简单的 SQL 层的执行逻辑下推到 KV 层执行，让 KV 层可以理解 Table 和 Column，可以批量读取多行结果，可以用 Where 里的 Expression 对结果进行过滤, 可以计算聚合函数，大幅减少了 RPC 次数和数据的传输量。
TiDB 的下推 API 通过把 SQL 层的计算下推到 KV 层，大幅减少 RPC 次数和数据传输量，使性能得到数量级的提升。但是当我们一开始启用下推 API 的时候，发现了一个问题，就是当事务写入了数据，但是还未提交的时候，又执行了 Select 操作。
这个时候，刚刚写入的未提交的脏数据读不到，得到的结果是错误的，比如我们在一个空表 t 执行：
begin; insert t values (1); select * from t; 这时我们期待的结果是一条记录 “1”，但是启用下推 API 后得到的结果是空。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.12】rocksdb 日志分析和性能调优经验</title>
      <link>https://pingcap.com/meetup/meetup-12-20160618/</link>
      <pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-12-20160618/</guid>
      <description>今天是 PingCAP 第12期 Meetup ，主题是张金鹏分享的《 rocksdb 日志分析和性能调优经验 》。
张金鹏 《 rocksdb 日志分析和性能调优经验 》 首先和大家一起分享如何分析 rocksdb 的 LOG，包括观察 compaction 相关的统计信息。例如每个 level 导致的 compaction 个数，每个 compaction job 的平均持续时长，compaction 导致的 read 总量和 write 量，以及写放大等；也可以观察整个系统是否有 stall 情况，持续多长时间，时间占比是多少等；另外，还有跟踪某个具体的 compaction job 的 input files 组成，output files，以及 compacting 过程中 drop 掉的 key 个数等信息。
然后根据 rocksdb 的 LOG 以及观察到的系统负载情况，来对不同参数组进行测试。
最后对比不同参数组的一些效果，包括同样的数据量导致的 compaction 放大比例；整个系统的 stall 情况；以及是否存在长时间的 compaction 导致的长时间高 CPU 及高 IO，从而对 TiKV 服务本身造成负面影响等情况。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.11】TiKV 的结构化存储模型优化 &amp; 深入解析 LevelDB</title>
      <link>https://pingcap.com/meetup/meetup-11-20160604/</link>
      <pubDate>Sat, 04 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-11-20160604/</guid>
      <description>今天是 PingCAP 第 11 期 Meetup ，主题是黄梦龙分享的《TiKV 的结构化存储模型优化》和张金鹏分享的《深入解析 LevelDB》。
黄梦龙《TiKV 的结构化存储模型优化》 目前 TiKV 的存储模型是简单的纯 Key-Value，在存储 SQL 结构化数据的过程中会产生比较严重的读写放大问题。我们计划为 TiKV 添加类似于 Hbase 的 ColumnFamily 机制，以使得 TiKV 与 TiDB 成为更加完美的搭档。大家对其中的实现细节，以及各种方案的优缺点进行了探讨。
张金鹏 《深入解析 LevelDB》 首先介绍了 LevelDB 的整体架构，以及 LSM Tree 这一数据库中非常经典的结构。之后对 LevelDB 的写和读的流程进行分析，同时介绍 LevelDB 的 snapshot 功能的实现原理，以及 iterator 内部实现，和 iterator 存在的潜在问题。最后介绍 LevelDB 的 compaction 过程，以及存在的问题。</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.10】TiKV 的网络模拟测试 &amp; TiDB 的条件下推优化</title>
      <link>https://pingcap.com/meetup/meetup-10-20160528/</link>
      <pubDate>Sat, 28 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-10-20160528/</guid>
      <description>今天是 PingCAP 第 10 期 Meetup ，跟京东小伙伴就 Raft group 中出现网络隔离时的 stale read 的问题做了充分讨论交流。之后进行的分享主题是《TiKV 的网络模拟测试》和《TiDB 的条件下推优化》。
随机讨论 Raft group 中出现网络隔离时，会有 stale read 的问题。目前我们考虑采用 region leader 的方案，保证在出现网络隔离的情况下，也能保证读的正确性。大家对其中的实现细节，以及各种方案的优缺点进行了讨论。
刘奇《TiKV 的网络模拟测试》 TiKV 如何做分布式系统测试。目前已经构建了一套测试框架，提供设置网络延迟、网络隔离、节点掉线等功能，用于构建测试用例。
周昱行《TiDB 的条件下推优化》 使用基于 Row 的 Merge 算法，解决存在脏数据时，使用 TiDB 下推 API 优化的问题。
TiDB 的下推 API 相比基础的 API 对读性能有着几个数量级的提升，任何无法使用下推 API 的操作的请求，性能都慢到完全无法接受的程度。但是之前的实现并不能保证所有读请求都可以走下推 API, 当事务有写操作以后，无法使用下推 API。无法使用的原因是，事务提交之前，事务内写入的数据是对事务自身是可见的，下推 API 只能读到已提交的数据，返回的结果是错误的。
一个很常见的场景是在一个事务内 UPDATE 多个 Row，会退化到使用基础 KV API。
本周 TiDB 的一个更新，通过设计实现了一种基于 Row 的 Merge 算法，解决了这个问题。
小花絮： 赠送 PingCAP 家褶皱版美背 T 恤买家秀一只。大家周末愉快 ;P</description>
    </item>
    
    <item>
      <title>【Infra Meetup No.9】SQL 子查询优化 &amp; TiKV MVCC 和 GC 实现</title>
      <link>https://pingcap.com/meetup/meetup-09-20160521/</link>
      <pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/meetup-09-20160521/</guid>
      <description>今天是 PingCAP 第9期 Meetup ，分享主题是韩飞的《 SQL 子查询优化》和刘奇的《 TiKV MVCC 和 GC 实现》。
韩飞《SQL 子查询优化》 分享 SQL subqueries 的变换和优化问题。关联子查询的优化是 SQL 优化中很重要的一部分，一般的执行方式方式是 correlated execution，但是可以通过引入 Apply 算子形式化证明所有的子查询都可以改写成 Join 的不同形式。在分布式场景下，Join 可以比 correlated execution 有更多的优化空间。
刘奇《TiKV MVCC 和 GC 实现》 详细分析了 TiKV 的 MVCC 机制, 事务模型，并进一步介绍了 percolator 事务模型的特点，以及对 GC 的影响。另外讲解了 TiKV 对 percolator 事务模型的改进, 以及 TiKV 的 GC 算法，和如何支持长时间的数据库备份和分析操作。</description>
    </item>
    
    <item>
      <title></title>
      <link>https://pingcap.com/meetup/list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/list/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://pingcap.com/meetup/recording/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://pingcap.com/meetup/recording/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>